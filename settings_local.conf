# @group: CLIENT_NAMES compact
clientName                                 = Not specified
clientName.dev.siggi                       = Siggi
clientName.dev.simon                       = Simon
clientName.dev.liam                        = Liam
clientName.dev.stu                         = Stu
clientName.dev.vicente                     = Vicente
clientName.dev.gokhan                      = Gokhan
clientName.dev.davide                      = Davide
clientName.dev.NEW_USER_TEMPLATE           = NEW_USER_TEMPLATE # template for future new users (referenced in documentation)
clientName.docker.host.ubsv1               = ubsv1
clientName.docker.host.ubsv2               = ubsv2
clientName.docker.host.ubsv3               = ubsv3
clientName.operator.scaling.m1             = M1
clientName.operator.scaling.m2             = M2
clientName.operator.scaling.m3             = M3
clientName.operator.scaling.m4             = M4
clientName.operator.scaling.m5             = M5
clientName.operator.scaling.m6             = M6
clientName.operator.teratestnet.prod.1     = prod-teranet-1
clientName.operator.teratestnet.prod.2     = prod-teranet-2
clientName.operator.teratestnet.prod.3     = prod-teranet-3
clientName.operator.teratestnet.staging.1  = staging-teranet-1
clientName.operator.teratestnet.staging.2  = staging-teranet-2
clientName.operator.teratestnet.staging.3  = staging-teranet-3
clientName.operator.teratestnet.tx-blaster = prod-teranet-1
clientName.operator.mainnet.euw1-1         = prod-mainnet-1
clientName.operator.mainnet.euw1-2         = prod-mainnet-2
clientName.operator.mainnet.euw1-3         = prod-mainnet-3
clientName.docker.ss.ubsv1                 = ubsv1
clientName.docker.ubsv1                    = ubsv1
clientName.docker.ubsv2                    = ubsv2
clientName.docker.ubsv3                    = ubsv3
clientName.docker.ci                       = ubsv1
clientName.docker.ci.ubsv1                 = ubsv1
clientName.docker.ci.ubsv2                 = ubsv2
clientName.docker.ci.ubsv3                 = ubsv3
# @endgroup

KAFKA_BLOCKS                         = blocks
KAFKA_BLOCKS.docker.ubsv1            = blocks1
KAFKA_BLOCKS.docker.ubsv2            = blocks2
KAFKA_BLOCKS.docker.ubsv3            = blocks3
KAFKA_BLOCKS.docker.host             = blocks${clientName}
KAFKA_BLOCKS.mainnet2                = svode-blocks
KAFKA_BLOCKS.docker.ss.ubsv1         = blocks1
KAFKA_BLOCKS.operator.mainnet.euw1-1 = blocks1
KAFKA_BLOCKS.operator.mainnet.euw1-2 = blocks2
KAFKA_BLOCKS.operator.mainnet.euw1-3 = blocks3
KAFKA_BLOCKS.operator.teratestnet    = blocks-${clientName}

KAFKA_BLOCKS_FINAL                         = blocks-final
KAFKA_BLOCKS_FINAL.docker.ubsv1            = blocks-final1
KAFKA_BLOCKS_FINAL.docker.ubsv2            = blocks-final2
KAFKA_BLOCKS_FINAL.docker.ubsv3            = blocks-final3
KAFKA_BLOCKS_FINAL.docker.host             = blocks-final${clientName}
KAFKA_BLOCKS_FINAL.mainnet2                = svnode-blocks-final
KAFKA_BLOCKS_FINAL.docker.ss.ubsv1         = blocks-final1
KAFKA_BLOCKS_FINAL.operator.mainnet.euw1-1 = blocks-final1
KAFKA_BLOCKS_FINAL.operator.mainnet.euw1-2 = blocks-final2
KAFKA_BLOCKS_FINAL.operator.mainnet.euw1-3 = blocks-final3
KAFKA_BLOCKS_FINAL.operator.teratestnet    = blocks-final-${clientName}

KAFKA_HOSTS.dev                 = 127.0.0.1:${KAFKA_PORT}
KAFKA_HOSTS.dev.liam            = localhost:${KAFKA_PORT}
KAFKA_HOSTS.docker              = kafka-shared:${KAFKA_PORT}
KAFKA_HOSTS.docker.ss.ubsv1     = kafka-shared:${KAFKA_PORT}
KAFKA_HOSTS.docker.host         = localhost:${KAFKA_PORT}
KAFKA_HOSTS.mainnet2            = b-2.ubsvmskeuc.hnkv05.c6.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-1.ubsvmskeuc.hnkv05.c6.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskeuc.hnkv05.c6.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.scaling.m1 = b-1.ubsvmskeu.4h864p.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-2.ubsvmskeu.4h864p.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskeu.4h864p.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.scaling.m2 = b-2.ubsvmskus.deer9v.c24.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskus.deer9v.c24.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-1.ubsvmskus.deer9v.c24.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.scaling.m3 = b-1.ubsvmskasia.o871j9.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-2.ubsvmskasia.o871j9.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskasia.o871j9.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.scaling.m4 = b-1.ubsvmskane.sz9taz.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-2.ubsvmskane.sz9taz.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskane.sz9taz.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.scaling.m5 = b-3.ubsvmskcan.f2xt8x.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-1.ubsvmskcan.f2xt8x.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-2.ubsvmskcan.f2xt8x.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.scaling.m6 = b-1.ubsvmskusw.gmnud6.c4.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-2.ubsvmskusw.gmnud6.c4.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskusw.gmnud6.c4.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.teratestnet= b-1.teranodeteranetprod.00boln.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-2.teranodeteranetprod.00boln.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.teranodeteranetprod.00boln.c3.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.operator.mainnet    = b-2.ubsvmskeuwest1kafka.86es5g.c6.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-3.ubsvmskeuwest1kafka.86es5g.c6.kafka.${regionName}.amazonaws.com:${KAFKA_PORT},b-1.ubsvmskeuwest1kafka.86es5g.c6.kafka.${regionName}.amazonaws.com:${KAFKA_PORT}
KAFKA_HOSTS.test                = 127.0.0.1:${KAFKA_PORT}

KAFKA_LEGACY_INV                         = legacy-inv
KAFKA_LEGACY_INV.docker.ci.ubsv1         = legacy-inv1
KAFKA_LEGACY_INV.docker.ci.ubsv2         = legacy-inv2
KAFKA_LEGACY_INV.docker.ci.ubsv3         = legacy-inv3
KAFKA_LEGACY_INV.docker.host             = legacy-inv${clientName}
KAFKA_LEGACY_INV.mainnet2                = svnode-legacy-inv
KAFKA_LEGACY_INV.docker.ss.ubsv1         = legacy-inv1
KAFKA_LEGACY_INV.operator.mainnet.euw1-1 = legacy-inv1
KAFKA_LEGACY_INV.operator.mainnet.euw1-2 = legacy-inv2
KAFKA_LEGACY_INV.operator.mainnet.euw1-3 = legacy-inv3
KAFKA_LEGACY_INV.operator.teratestnet    = legacy-inv-${clientName}

KAFKA_PARTITIONS_LOW                    = 1
KAFKA_PARTITIONS_HIGH                   = 8
KAFKA_PARTITIONS_HIGH.operator          = 512
KAFKA_PARTITIONS_HIGH.operator.teratestnet  = 32

KAFKA_PORT                   = 9092
KAFKA_PORT.dev.kafkatool     = 9094
KAFKA_PORT.docker.host       = 19092
KAFKA_PORT.docker.ubsv1.tec7 = 9999

KAFKA_REJECTEDTX                         = rejectedtx
KAFKA_REJECTEDTX.docker.ubsv1            = rejectedtx1
KAFKA_REJECTEDTX.docker.ubsv2            = rejectedtx2
KAFKA_REJECTEDTX.docker.ubsv3            = rejectedtx3
KAFKA_REJECTEDTX.docker.host             = rejectedtx${clientName}
KAFKA_REJECTEDTX.mainnet2                = svnode-rejectedtx
KAFKA_REJECTEDTX.docker.ss.ubsv1         = rejectedtx1
KAFKA_REJECTEDTX.operator.mainnet.euw1-1 = rejectedtx1
KAFKA_REJECTEDTX.operator.mainnet.euw1-2 = rejectedtx2
KAFKA_REJECTEDTX.operator.mainnet.euw1-3 = rejectedtx3
KAFKA_REJECTEDTX.operator.teratestnet    = rejectedtx-${clientName}

# use the same kafka cluster for the testing env
KAFKA_REPLICATION_FACTOR          = 1
KAFKA_REPLICATION_FACTOR.operator = 3

KAFKA_SUBTREES                         = subtrees
KAFKA_SUBTREES.docker.ubsv1            = subtrees1
KAFKA_SUBTREES.docker.ubsv2            = subtrees2
KAFKA_SUBTREES.docker.ubsv3            = subtrees3
KAFKA_SUBTREES.docker.host             = subtrees${clientName}
KAFKA_SUBTREES.mainnet2                = svode-subtrees
KAFKA_SUBTREES.docker.ss.ubsv1         = subtrees1
KAFKA_SUBTREES.operator.mainnet.euw1-1 = subtrees1
KAFKA_SUBTREES.operator.mainnet.euw1-2 = subtrees2
KAFKA_SUBTREES.operator.mainnet.euw1-3 = subtrees3
KAFKA_SUBTREES.operator.teratestnet    = subtrees-${clientName}

KAFKA_TXMETA                         = txmeta
KAFKA_TXMETA.docker.ubsv1            = txmeta1
KAFKA_TXMETA.docker.ubsv2            = txmeta2
KAFKA_TXMETA.docker.ubsv3            = txmeta3
KAFKA_TXMETA.docker.host             = txmeta${clientName}
KAFKA_TXMETA.mainnet2                = svnode-txmeta
KAFKA_TXMETA.docker.ss.ubsv1         = txmeta1
KAFKA_TXMETA.operator.mainnet.euw1-1 = txmeta1
KAFKA_TXMETA.operator.mainnet.euw1-2 = txmeta2
KAFKA_TXMETA.operator.mainnet.euw1-3 = txmeta3
KAFKA_TXMETA.operator.teratestnet    = txmeta-${clientName}

KAFKA_UNITTEST = unittest

KAFKA_VALIDATORTXS                         = validatortxs
KAFKA_VALIDATORTXS.docker.ubsv1            = validatortxs1
KAFKA_VALIDATORTXS.docker.ubsv2            = validatortxs2
KAFKA_VALIDATORTXS.docker.ubsv3            = validatortxs3
KAFKA_VALIDATORTXS.docker.host             = validatortxs${clientName}
KAFKA_VALIDATORTXS.docker.ss.ubsv1         = validatortxs1
KAFKA_VALIDATORTXS.operator.mainnet.euw1-1 = validatortxs1
KAFKA_VALIDATORTXS.operator.mainnet.euw1-2 = validatortxs2
KAFKA_VALIDATORTXS.operator.mainnet.euw1-3 = validatortxs3
KAFKA_VALIDATORTXS.operator.teratestnet    = validatortxs-${clientName}

# @group: PORT PREFIXES compact
PORT_PREFIX                                   =
PORT_PREFIX.docker.ci.externaltxblaster.ubsv1 = 1
PORT_PREFIX.docker.ci.externaltxblaster.ubsv2 = 2
PORT_PREFIX.docker.ci.externaltxblaster.ubsv3 = 3
PORT_PREFIX.docker.host.ubsv1                 = 1
PORT_PREFIX.docker.host.ubsv2                 = 2
PORT_PREFIX.docker.host.ubsv3                 = 3

# @endgroup

# @group: PORTS compact
ALERT_P2P_PORT               = 9908 # this is set in the mainnet.json in services/alert/config
ASSET_HTTP_PORT              = 8090
BLOCKCHAIN_GRPC_PORT         = 8087
BLOCKCHAIN_HTTP_PORT         = 8082
BLOCK_ASSEMBLY_GRPC_PORT     = 8085
BLOCK_PERSISTER_HTTP_PORT    = 8083
BLOCK_VALIDATION_GRPC_PORT   = 8088
BLOCK_VALIDATION_HTTP_PORT   = 8188
CENTRIFUGE_PORT              = 8892
COINBASE_GRPC_PORT           = 8093
FAUCET_HTTP_PORT             = 8097
HEALTH_CHECK_PORT            = 8000
JAEGER_PORT                  = 6831
JAEGER_PORT_HTTP             = 4318
LEGACY_GRPC_PORT             = 8099
LEGACY_HTTP_PORT             = 8098
P2P_BOOTSTRAP_PORT           = 9901
P2P_GRPC_PORT                = 9904
P2P_HTTP_PORT                = 9906
P2P_PORT                     = 9905
P2P_PORT_COINBASE            = 9907
PROFILE_PORT                 = 9091
PROFILE_PORT_TXBLASTER       = 9092
PROPAGATION_GRPC_PORT        = 8084
PROPAGATION_HTTP_PORT        = 8833
PROPAGATION_QUIC_PORT        = 8384
REDIS_PORT                   = 6379
RPC_PORT                     = 9292
SUBTREE_VALIDATION_GRPC_PORT = 8086
VALIDATOR_GRPC_PORT          = 8081
# @endgroup

REDIS_HOSTS          = localhost:${REDIS_PORT}
REDIS_HOSTS.operator = ubsv-store-redis-cluster-0.ubsv-store-redis-cluster-headless.redis.svc.cluster.local:$},ubsv-store-redis-cluster-1.ubsv-store-redis-cluster-headless.redis.svc.cluster.local:$},ubsv-store-redis-cluster-2.ubsv-store-redis-cluster-headless.redis.svc.cluster.local:$},ubsv-store-redis-cluster-3.ubsv-store-redis-cluster-headless.redis.svc.cluster.local:$},ubsv-store-redis-cluster-4.ubsv-store-redis-cluster-headless.redis.svc.cluster.local:$},ubsv-store-redis-cluster-5.ubsv-store-redis-cluster-headless.redis.svc.cluster.local:$}

# @group: REGION_NAMES compact
# todo joe to standardize region names
regionName.mainnet2             = eu-central-1
regionName.operator.scaling.m1  = eu-west-1
regionName.operator.scaling.m2  = us-east-1
regionName.operator.scaling.m3  = ap-south-1
regionName.operator.scaling.m4  = ap-northeast-2
regionName.operator.scaling.m5  = ca-central-1
regionName.operator.scaling.m6  = us-west-2
regionName.operator.teratestnet = eu-central-1
regionName.operator.mainnet     = eu-west-1
# @endgroup

advertisingInterval = 10s

# Advertising Configuration
# -------------------------
advertisingURL =

aerospike_debug = false

aerospike_host.docker.ubsv1    = aerospike-1
aerospike_host.docker.ubsv2    = aerospike-2
aerospike_host.docker.ubsv3    = aerospike-3
aerospike_host.docker.ci       = localhost
aerospike_host.docker.ci.ubsv1 = aerospike-1
aerospike_host.docker.ci.ubsv2 = aerospike-2
aerospike_host.docker.ci.ubsv3 = aerospike-3

# @group: aerospike_policies compact
# The following 3 policies are used for all read/write operations when the aerospike_useDefaultPolicies is false
# SleepBetweenRetries is not being used in the current implementation
# SleepMultiplier is not being used in the current implementation
# ExitFastOnExhaustedConnectionPool is not being used in the current implementation
aerospike_batchPolicy = aerospike:///?MaxRetries=5&SleepBetweenRetries=500ms&SleepMultiplier=1&TotalTimeout=64s&SocketTimeout=10s&ConcurrentNodes=0
aerospike_readPolicy  = aerospike:///?MaxRetries=5&SleepBetweenRetries=500ms&SleepMultiplier=1&TotalTimeout=1s&SocketTimeout=1s
aerospike_writePolicy = aerospike:///?MaxRetries=5&SleepBetweenRetries=500ms&SleepMultiplier=1&TotalTimeout=1s&SocketTimeout=1s
# @endgroup

aerospike_port.docker.ubsv1      = 3100
aerospike_port.docker.ubsv2      = 3200
aerospike_port.docker.ubsv3      = 3300
aerospike_port.docker.ci         = 13100
aerospike_port.docker.ci.ubsv1   = 13100
aerospike_port.docker.ci.ubsv2   = 13200
aerospike_port.docker.ci.ubsv3   = 13300
aerospike_port.docker.ubsv1.test = 13100
aerospike_port.docker.ubsv2.test = 13200
aerospike_port.docker.ubsv3.test = 13300

aerospike_useDefaultBasePolicies = false

aerospike_useDefaultPolicies = false

aerospike_warmUp = true

alert_genesis_keys = "02a1589f2c8e1a4e7cbf28d4d6b676aa2f30811277883211027950e82a83eb2768 | 03aec1d40f02ac7f6df701ef8f629515812f1bcd949b6aa6c7a8dd778b748b2433 | 03ddb2806f3cc48aa36bd4aea6b9f1c7ed3ffc8b9302b198ca963f15beff123678 | 036846e3e8f4f944af644b6a6c6243889dd90d7b6c3593abb9ccf2acb8c9e606e2 | 03e45c9dd2b34829c1d27c8b5d16917dd0dc2c88fa0d7bad7bffb9b542229a9304"

alert_p2p_private_key = "e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526"

alert_protocol_id = "/bitcoin/alert-system/1.0.0"

alert_store                      = sqlite:///alert



alert_topic_name = "bitcoin_alert_system"

asset_apiPrefix = /api/v1

asset_centrifugeListenAddress             = :${CENTRIFUGE_PORT}
asset_centrifugeListenAddress.dev         = localhost:${CENTRIFUGE_PORT}
asset_centrifugeListenAddress.docker.host = localhost:${PORT_PREFIX}${CENTRIFUGE_PORT}

# turn this on to activate the centrifuge server
asset_centrifuge_disable          = false
asset_centrifuge_disable.dev.liam         = true
asset_centrifuge_disable.docker.m = true
asset_centrifuge_disable.operator = true

asset_httpAddress                             = http://localhost:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.dev.siggi                   = http://bastion.ubsv.dev:18090${asset_apiPrefix}
asset_httpAddress.dev.simon                   = http://bastion.ubsv.dev:18190${asset_apiPrefix}
asset_httpAddress.dev.liam                    = http://bastion.ubsv.dev:18290${asset_apiPrefix}
asset_httpAddress.dev.stu                     = http://bastion.ubsv.dev:18390${asset_apiPrefix}
asset_httpAddress.dev.vicente                 = http://bastion.ubsv.dev:18490${asset_apiPrefix}
asset_httpAddress.dev.gokhan                  = http://bastion.ubsv.dev:18590${asset_apiPrefix}
asset_httpAddress.dev.NEW_USER_TEMPLATE       = http://bastion.ubsv.dev:18x90 # template for future new users (referenced in documentation)
asset_httpAddress.docker.m                    = http://ubsv-asset:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker                      = http://${clientName}:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.ci.externaltxblaster = http://localhost:${PORT_PREFIX}${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.host                 = http://localhost:${PORT_PREFIX}${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.docker.ss.ubsv1             = http://asset-1:${ASSET_HTTP_PORT}${asset_apiPrefix}
asset_httpAddress.operator.teratestnet        = http://${clientName}.teratestnet.ubsv.dev${asset_apiPrefix}
asset_httpAddress.operator.scaling            = http://${clientName}.scaling.ubsv.dev${asset_apiPrefix}

asset_httpListenAddress                   = :${ASSET_HTTP_PORT}
asset_httpListenAddress.dev               = localhost:${ASSET_HTTP_PORT}
asset_httpListenAddress.docker.host       = localhost:${PORT_PREFIX}${ASSET_HTTP_PORT}
asset_httpListenAddress.docker.ubsv1.tec6 =

asset_http_port = 80

asset_https_port = 443

blockMinedCacheMaxMB        = 256
blockMinedCacheMaxMB.docker = 32

blockPersisterStore                         = ${blockstore}
blockPersisterStore.operator.mainnet.euw1-1 = http://localhost:${BLOCK_PERSISTER_HTTP_PORT}

blockPersister_httpListenAddress             = :${BLOCK_PERSISTER_HTTP_PORT}
blockPersister_httpListenAddress.docker      = http://localhost:${BLOCK_PERSISTER_HTTP_PORT}
blockPersister_httpListenAddress.docker.host = http://localhost:${PORT_PREFIX}${BLOCK_PERSISTER_HTTP_PORT}

blockPersister_stateFile                         = ./data/blockpersister_state.txt
blockPersister_stateFile.operator.mainnet.euw1-1 = /data/mainnet-1/blockpersister_state.txt

blockPersister_persistSleep                     = 1m
blockPersister_persistSleep.docker              = 10ms
blockPersister_persistAge                       = 100
blockPersister_persistAge.docker                = 0

block_checkDuplicateTransactionsConcurrency.operator = 32

# block concurrency settings
block_getAndValidateSubtreesConcurrency.operator = 32

block_validOrderAndBlessedConcurrency.operator = 32

blockassembly_disabled                                  = false
blockassembly_disabled.docker.ubsv2.test.resilience.tc2 = true

blockassembly_grpcAddress                   = localhost:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.m          = ubsv-blockassembly:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker            = ${clientName}:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.host       = localhost:${PORT_PREFIX}${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.operator          = k8s:///block-assembly.${clientName}.svc.cluster.local:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.ss.ubsv1   = blockassembly-1:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcAddress.docker.ubsv1.tec3 =

blockassembly_grpcListenAddress             = :${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcListenAddress.dev         = localhost:${BLOCK_ASSEMBLY_GRPC_PORT}
blockassembly_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${BLOCK_ASSEMBLY_GRPC_PORT}

blockassembly_grpcMaxRetries                      = 3
blockassembly_grpcMaxRetries.operator.teratestnet = 5

blockassembly_grpcRetryBackoff                      = 2s
blockassembly_grpcRetryBackoff.operator.teratestnet = 5s

# the local TTL cache is only used when remote TTL stores are enabled
blockassembly_localTTLCache =

blockassembly_maxBlockReorgCatchup = 100

blockassembly_maxBlockReorgRollback = 100

blockassembly_moveDownBlockConcurrency.operator = 375

blockassembly_processRemainderTxHashesConcurrency.operator = 375

blockassembly_sendBatchSize          = 100
blockassembly_sendBatchSize.operator = 1024

blockassembly_sendBatchTimeout          = 2
blockassembly_sendBatchTimeout.operator = 20

blockassembly_subtreeProcessorBatcherSize                      = 1000
blockassembly_subtreeProcessorBatcherSize.operator             = 10000000
blockassembly_subtreeProcessorBatcherSize.operator.teratestnet = 100000

blockassembly_subtreeProcessorConcurrentReads.operator = 375

blockassembly_subtreeTTL.docker.host = 0

blockassembly_difficultyCache = true

blockchain_grpcAddress                 = localhost:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.m        = ubsv-blockchain:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker          = ${clientName}:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.host     = localhost:${PORT_PREFIX}${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.docker.ss.ubsv1 = blockchain-1:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcAddress.operator        = k8s:///blockchain.${clientName}.svc.cluster.local:${BLOCKCHAIN_GRPC_PORT}

blockchain_grpcListenAddress                    = :${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker             = 0.0.0.0:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker.host        = localhost:${PORT_PREFIX}${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.dev                = localhost:${BLOCKCHAIN_GRPC_PORT}
blockchain_grpcListenAddress.docker.ubsv3.debug = :${BLOCKCHAIN_GRPC_PORT}

blockchain_httpListenAddress             = :${BLOCKCHAIN_HTTP_PORT}
blockchain_httpListenAddress.dev         = localhost:${BLOCKCHAIN_HTTP_PORT}
blockchain_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${BLOCKCHAIN_HTTP_PORT}

# Blockchain Service Configuration
# --------------------------------
blockchain_maxRetries.docker.host = 3

# @group: blockchain_store compact
blockchainDB.docker.ubsv1           = ubsv1
blockchainDB.docker.ubsv2           = ubsv2
blockchainDB.docker.ubsv3           = ubsv3
blockchainDBUserPwd.docker.ubsv1    = miner1
blockchainDBUserPwd.docker.ubsv2    = miner2
blockchainDBUserPwd.docker.ubsv3    = miner3
# @endgroup

blockchain_store                                      = sqlite:///blockchain
blockchain_store.dev.stu                              = postgres://ubsv:ubsv@localhost:5432/ubsv
blockchain_store.dev.simon                            = postgres://ubsv:ubsv@localhost:5432/ubsv
blockchain_store.dev.liam                             = postgres://ubsv:ubsv@localhost:5432/ubsv
blockchain_store.dev.legacy                           = postgres://ubsv:ubsv@localhost:5432/ubsv
blockchain_store.mainnet                              = postgres://ubsv:ubsv@localhost:5432/ubsv
blockchain_store.mainnet2                             = postgres://ubsv:ubsv@localhost:5432/ubsv
blockchain_store.docker.ci.chainintegrity.ubsv1       = postgres://miner1:miner1@localhost:5432/ubsv1
blockchain_store.docker.ci.chainintegrity.ubsv2       = postgres://miner2:miner2@localhost:5432/ubsv2
blockchain_store.docker.ci.chainintegrity.ubsv3       = postgres://miner3:miner3@localhost:5432/ubsv3
blockchain_store.docker.host.ubsv1                    = postgres://miner1:miner1@localhost:15432/ubsv1
blockchain_store.docker.host.ubsv2                    = postgres://miner2:miner2@localhost:15432/ubsv2
blockchain_store.docker.host.ubsv3                    = postgres://miner3:miner3@localhost:15432/ubsv3
blockchain_store.docker.m                             = postgres://miner1:miner1@postgres:5432/ubsv1
blockchain_store.docker.ss.ubsv1                      = postgres://miner1:miner1@postgres:5432/ubsv1


blockchain_store.docker                               = postgres://${blockchainDBUserPwd}:${blockchainDBUserPwd}@postgres:5432/${blockchainDB}
blockchain_store.docker.ubsv1.test.context.testrunner = postgres://miner1:miner1@localhost:7432/ubsv1
blockchain_store.docker.ubsv2.test.context.testrunner = postgres://miner2:miner2@localhost:7432/ubsv2
blockchain_store.docker.ubsv3.test.context.testrunner = postgres://miner3:miner3@localhost:7432/ubsv3
blockchain_store.docker.ubsv1.context.testrunner      = postgres://${blockchainDBUserPwd}:${blockchainDBUserPwd}@localhost:5432/${blockchainDB}

blockchain_store_cache_enabled = true

# Set maximum block size in bytes we will mine. Size of the mined block will never exceed the maximum block size we will accept (-excessiveblocksize)
blockmaxsize = 0 # The value may be given in bytes or with unit (B, kB, MB, GB)

blockstore                                      = file://./data/blockstore?localTTLStore=file&localTTLStorePath=data/blockstore-ttl-1 | data/blockstore-ttl-2
blockstore.dev                                  = null:///?localTTLStore=file&localTTLStorePath=./data/blockstore-ttl
blockstore.dev.system.test                      = file://./data/blockstore
blockstore.dev.simon                            = file://./data/blockstore
blockstore.docker                               = file://./data/blockstore
blockstore.docker.host                          = file://./data/${clientName}/blockstore
blockstore.mainnet                              = file://./data/blockstore
blockstore.mainnet2                             = file:///data/ibd/blockstore
blockstore.operator                             = file:///data/${clientName}/blockstore
blockstore.operator.teratestnet                 = lustre:///?region=${regionName}&localDir=/data/${clientName}/blockstore&localPersist=s3
blockstore.docker.ubsv1.test.context.testrunner = file://./../../data/test/ubsv1/blockstore
blockstore.docker.ubsv2.test.context.testrunner = file://./../../data/test/ubsv2/blockstore
blockstore.docker.ubsv3.test.context.testrunner = file://./../../data/test/ubsv3/blockstore
blockstore.docker.ubsv1.context.testrunner      = file://./../../data/ubsv1/blockstore
blockstore.docker.ubsv2.context.testrunner      = file://./../../data/ubsv2/blockstore
blockstore.docker.ubsv3.context.testrunner      = file://./../../data/ubsv3/blockstore

# quick validation means it only uses the txmeta cache
# (ignored if blockvalidation_validation_max_retries is 0)
blockvalidation_fail_fast_validation        = true
blockvalidation_fail_fast_validation.docker = false

# limit this, this is happening in the background
blockvalidation_finalizeBlockValidationConcurrency = 8

blockvalidation_getMissingTransactions = 32

blockvalidation_grpcAddress                      = localhost:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.docker.m             = ubsv-blockvalidation:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.docker.ubsv1.tec4    =
blockvalidation_grpcAddress.docker.host          = localhost:${PORT_PREFIX}${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.operator             = k8s:///block-validation.${clientName}.svc.cluster.local:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcAddress.docker.ci.ubsv1.tec4 =

blockvalidation_grpcListenAddress                      = :${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcListenAddress.dev                  = localhost:${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcListenAddress.docker.host          = localhost:${PORT_PREFIX}${BLOCK_VALIDATION_GRPC_PORT}
blockvalidation_grpcListenAddress.docker.ubsv1.tec4    =
blockvalidation_grpcListenAddress.docker.ci.ubsv1.tec4 =

blockvalidation_httpAddress                      = http://localhost:${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpAddress.docker.m             = http://ubsv-blockvalidation:${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpAddress.docker               = http://${clientName}:${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpAddress.docker.host          = http://localhost:${PORT_PREFIX}${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpAddress.docker.ss.ubsv1      = http://blockvalidation-1:${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpAddress.docker.ubsv1.tec4    =
blockvalidation_httpAddress.operator             = http://block-validation.${clientName}.svc.cluster.local:${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpAddress.docker.ci.ubsv1.tec4 =

blockvalidation_httpListenAddress                      = :${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpListenAddress.dev                  = localhost:${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpListenAddress.docker.host          = localhost:${PORT_PREFIX}${BLOCK_VALIDATION_HTTP_PORT}
blockvalidation_httpListenAddress.docker.ubsv1.tec4    =
blockvalidation_httpListenAddress.docker.ci.ubsv1.tec4 =

blockvalidation_localSetTxMinedConcurrency = 8

blockvalidation_maxPreviousBlockHeadersToCheck = 100

# too big a batch and you get asset service timeout challenges
blockvalidation_missingTransactionsBatchSize = 5000

blockvalidation_processTxMetaUsingCache_BatchSize = 1024

blockvalidation_processTxMetaUsingCache_Concurrency = 32

# cache miss threshold is the number of missing txs to allow before we fail
blockvalidation_processTxMetaUsingCache_MissingTxThreshold = 1

blockvalidation_processTxMetaUsingStore_BatchSize = 1024

blockvalidation_processTxMetaUsingStore_Concurrency = 32

# store miss threshold is the number of missing txs when searching our store, ignored during BlockVaidation
blockvalidation_processTxMetaUsingStore_MissingTxThreshold = 1

# TODO - remove this when we have a better solution
blockvalidation_skipCheckParentMined.operator.mainnet.euw1-1 = true

# block validation concurrency settings
# this is basically making subtree processing unlimited
# it is very important that new subtrees are processed as fast as possible
blockvalidation_subtreeFoundChConcurrency          = 1
blockvalidation_subtreeFoundChConcurrency.docker   = 1
blockvalidation_subtreeFoundChConcurrency.operator = 128

# limit this, this is happening in the background
blockvalidation_subtreeTTLConcurrency = 8

# abandon subtree validation when missing total exceeds threshold, ignored during BlockValidation
blockvalidation_subtree_validation_abandon_threshold          = 1
blockvalidation_subtree_validation_abandon_threshold.operator = 1000

blockvalidation_validateBlockSubtreesConcurrency = 32

blockvalidation_validation_max_retries = 3

blockvalidation_validation_retry_sleep = 5s

coinbaseDB.docker.ubsv1    = coinbase1
coinbaseDB.docker.ubsv2    = coinbase2
coinbaseDB.docker.ubsv3    = coinbase3
coinbaseDB.docker.ci       = coinbase1
coinbaseDB.docker.ci.ubsv1 = coinbase1
coinbaseDB.docker.ci.ubsv2 = coinbase2
coinbaseDB.docker.ci.ubsv3 = coinbase3

coinbaseDBUserPwd.docker.ubsv1    = coinbase1
coinbaseDBUserPwd.docker.ubsv2    = coinbase2
coinbaseDBUserPwd.docker.ubsv3    = coinbase3
coinbaseDBUserPwd.docker.ci       = coinbase1
coinbaseDBUserPwd.docker.ci.ubsv1 = coinbase1
coinbaseDBUserPwd.docker.ci.ubsv2 = coinbase2
coinbaseDBUserPwd.docker.ci.ubsv3 = coinbase3

coinbase_arbitrary_text                       = /teranode/
coinbase_arbitrary_text.dev.siggi             = /siggi/
coinbase_arbitrary_text.dev.simon             = /simon/
coinbase_arbitrary_text.dev.liam              = /liam/
coinbase_arbitrary_text.dev.stu               = /stu/
coinbase_arbitrary_text.dev.leonard           = /leonard/
coinbase_arbitrary_text.dev.vicente           = /vicente/
coinbase_arbitrary_text.dev.gokhan            = /gokhan/
coinbase_arbitrary_text.dev.davide            = /davide/
coinbase_arbitrary_text.dev.NEW_USER_TEMPLATE = /NEW_USER_TEMPLATE/ # template for future new users (referenced in documentation)
coinbase_arbitrary_text.docker.ubsv1          = /m1-eu/
coinbase_arbitrary_text.docker.ubsv2          = /m2-us/
coinbase_arbitrary_text.docker.ubsv3          = /m3-asia/
coinbase_arbitrary_text.docker.host           = /${clientName}/
coinbase_arbitrary_text.operator.scaling.m1   = /m1-eu/
coinbase_arbitrary_text.operator.scaling.m2   = /m2-us/
coinbase_arbitrary_text.operator.scaling.m3   = /m3-asia/
coinbase_arbitrary_text.operator.scaling.m4   = /m4-ane/
coinbase_arbitrary_text.operator.scaling.m5   = /m5-cc1/
coinbase_arbitrary_text.operator.scaling.m6   = /m6-usw/
coinbase_arbitrary_text.operator.teratestnet  = /${clientName}-euc/
coinbase_arbitrary_text.docker.ss.ubsv1       = /m1-eu/

coinbase_grpcAddress                             = # This should be empty by default
coinbase_grpcAddress.dev                         = localhost:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.ci.externaltxblaster = localhost:${PORT_PREFIX}${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.m                    = ubsv-coinbase:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker                      = ${clientName}:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.host                 = localhost:${PORT_PREFIX}${COINBASE_GRPC_PORT}
coinbase_grpcAddress.docker.ss.ubsv1             = coinbase-1:${COINBASE_GRPC_PORT}
coinbase_grpcAddress.operator.teratestnet        = k8s:///coinbase.${clientName}.svc.cluster.local:${COINBASE_GRPC_PORT}

coinbase_grpcListenAddress                             = :${COINBASE_GRPC_PORT}
coinbase_grpcListenAddress.dev                         = localhost:${COINBASE_GRPC_PORT}
coinbase_grpcListenAddress.docker.ci.externaltxblaster = localhost:${PORT_PREFIX}${COINBASE_GRPC_PORT}
coinbase_grpcListenAddress.docker.host                 = localhost:${PORT_PREFIX}${COINBASE_GRPC_PORT}

coinbase_notification_threshold.dev.stu  = 100000
coinbase_notification_threshold.operator = 100000

coinbase_p2p_peer_id.dev                            = 12D3KooWBBV8PL949p46DJHwJkjESoPGCYhqHv1Ek1DkbQ6HGB8X
coinbase_p2p_peer_id.docker.ubsv1                   = 12D3KooWNQWh27xAsZRuXzANGQjLVJqXGVdp1errjLfc3wWvawZw
coinbase_p2p_peer_id.docker.ubsv2                   = 12D3KooWNhWUxABRjenSeCT3V4zVKnPqfSA3jvXQnPbVmcp1ZtYU
coinbase_p2p_peer_id.docker.ubsv3                   = 12D3KooWS6HPmwhqSDdS78rLqUQpM39Jf59XYGxJNE77W4WziGL6
coinbase_p2p_peer_id.docker.ss.ubsv1                = 12D3KooWNQWh27xAsZRuXzANGQjLVJqXGVdp1errjLfc3wWvawZw
coinbase_p2p_peer_id.docker.host.ubsv1              = 12D3KooWNQWh27xAsZRuXzANGQjLVJqXGVdp1errjLfc3wWvawZw
coinbase_p2p_peer_id.docker.host.ubsv2              = 12D3KooWNhWUxABRjenSeCT3V4zVKnPqfSA3jvXQnPbVmcp1ZtYU
coinbase_p2p_peer_id.docker.host.ubsv3              = 12D3KooWS6HPmwhqSDdS78rLqUQpM39Jf59XYGxJNE77W4WziGL6
coinbase_p2p_peer_id.operator.scaling.m1            = 12D3KooWLTjEjjK323Wj48QreWQ729mPcS9mpX2jNCfG4SN5n8nM
coinbase_p2p_peer_id.operator.scaling.m2            = 12D3KooWCNGgnN3ZoZ4ZJg7i1tmYiisKdMhxUYTmYnFKP6XPPoxu
coinbase_p2p_peer_id.operator.scaling.m3            = 12D3KooWAF7kFdATDwtFgsoKep7obFFLjjodS7YdFUSErK7ZSoSP
coinbase_p2p_peer_id.operator.scaling.m4            = 12D3KooWKvjwjFesTV7PgNJ928JENGgfvhFXV4FCkVnPHzsYye5S
coinbase_p2p_peer_id.operator.scaling.m5            = 12D3KooWFDEoEPHTybXRt5FvjfxmdT8UQ94xygvX1TVptzcg2gTY
coinbase_p2p_peer_id.operator.scaling.m6            = 12D3KooWM55qLHkPW4qpQa5P44mWbgi9fGa9QzUdvx25aqAzyu2H
coinbase_p2p_peer_id.operator.teratestnet.prod.1    = 12D3KooWLTjEjjK323Wj48QreWQ729mPcS9mpX2jNCfG4SN5n8nM
coinbase_p2p_peer_id.operator.teratestnet.prod.2    = 12D3KooWCNGgnN3ZoZ4ZJg7i1tmYiisKdMhxUYTmYnFKP6XPPoxu
coinbase_p2p_peer_id.operator.teratestnet.prod.3    = 12D3KooWAF7kFdATDwtFgsoKep7obFFLjjodS7YdFUSErK7ZSoSP
coinbase_p2p_peer_id.operator.teratestnet.staging.1 = 12D3KooWKvjwjFesTV7PgNJ928JENGgfvhFXV4FCkVnPHzsYye5S
coinbase_p2p_peer_id.operator.teratestnet.staging.2 = 12D3KooWFDEoEPHTybXRt5FvjfxmdT8UQ94xygvX1TVptzcg2gTY
coinbase_p2p_peer_id.operator.teratestnet.staging.3 = 12D3KooWM55qLHkPW4qpQa5P44mWbgi9fGa9QzUdvx25aqAzyu2H

coinbase_p2p_private_key.dev                            = 44a5a189fbad1d7bc0c59b33fbd5e485f2f4d3d8bf293838c56ce72e53b557171444c0bb7d5cf75112717084cee9e9e98651421b3cd29d721e43c0a51d81aa54
coinbase_p2p_private_key.docker.ubsv1                   = e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526
coinbase_p2p_private_key.docker.ubsv2                   = 860616e0492a3050aa760440469acfe4f57cf5387a765f5227603c4f6aeac985bf6643d453a1d68a101e52766e9feb9721b95e34aa73e5ea6c69a44be43cab6d
coinbase_p2p_private_key.docker.ubsv3                   = 1d6a9c8963fdbb86eabc4d10cb1efdf418197cfc3f9779e3c8229663411ae5c8f1cee260eeeae89cb45aae6955230557eba5bf63ef38087ec6be91ab744326c7
coinbase_p2p_private_key.docker.ss.ubsv1                = e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526
coinbase_p2p_private_key.docker.host.ubsv1              = e76c77795b43d2aacd564648bffebde74a4c31540357dad4a3694a561b4c4f1fbb0ba060a3015f7f367742500ef8486707e58032af1b4dfdb1203c790bcf2526
coinbase_p2p_private_key.docker.host.ubsv2              = 860616e0492a3050aa760440469acfe4f57cf5387a765f5227603c4f6aeac985bf6643d453a1d68a101e52766e9feb9721b95e34aa73e5ea6c69a44be43cab6d
coinbase_p2p_private_key.docker.host.ubsv3              = 1d6a9c8963fdbb86eabc4d10cb1efdf418197cfc3f9779e3c8229663411ae5c8f1cee260eeeae89cb45aae6955230557eba5bf63ef38087ec6be91ab744326c7
coinbase_p2p_private_key.operator.scaling.m1            = 8b0681bab86242bc1d76c94ad44db783009414c564286b0108ce0e03a8fcc7d39e272e8bbc72dde3411386a3c0111b3e14c7bc37dd5b76f3c843ec5e7ad7546a
coinbase_p2p_private_key.operator.scaling.m2            = 8e5f7dad838fe46138f43e0f3cf2dab46693efd07b92e2897d486e987528f22925e3c2d8a842c3dc0432525373d982f10eb1b471381a3b6dead51df3d28d60f2
coinbase_p2p_private_key.operator.scaling.m3            = b233925e84dafe385cf5dadc27d70f3d2672580ba48db42cb937d54b212dde1d06573681bea355b305ea79843782dddfa316875ae054950d8822d8b0cbbe39e0
coinbase_p2p_private_key.operator.scaling.m4            = ebf5d29d076b3f9dbaab80b9535f1dfd7dfab579fdbed36cf2944e3073575dc29636f9d285dd5738b3b21eb5e49f19b4128fe888f38fd63d3a51abac47bb5a65
coinbase_p2p_private_key.operator.scaling.m5            = 953a0f178eda5847362eab86b90b794d720c261cf1218f1e2015e9af62f336d05026757c0e9c52821380561053e65e3b325886adbe7a2fb4e1a61c70dffa7db7
coinbase_p2p_private_key.operator.scaling.m6            = ce49a558c46012d4aaaacea957839be4fdd3298ad9c7bdd7a83d6b48aed0b58fa735cc746ff78e02f2009669457a63074d3d188abed260f718b3c9d6a81e580a
coinbase_p2p_private_key.operator.teratestnet.prod.1    = 8b0681bab86242bc1d76c94ad44db783009414c564286b0108ce0e03a8fcc7d39e272e8bbc72dde3411386a3c0111b3e14c7bc37dd5b76f3c843ec5e7ad7546a
coinbase_p2p_private_key.operator.teratestnet.prod.2    = 8e5f7dad838fe46138f43e0f3cf2dab46693efd07b92e2897d486e987528f22925e3c2d8a842c3dc0432525373d982f10eb1b471381a3b6dead51df3d28d60f2
coinbase_p2p_private_key.operator.teratestnet.prod.3    = b233925e84dafe385cf5dadc27d70f3d2672580ba48db42cb937d54b212dde1d06573681bea355b305ea79843782dddfa316875ae054950d8822d8b0cbbe39e0
coinbase_p2p_private_key.operator.teratestnet.staging.1 = b233925e84dafe385cf5dadc27d70f3d2672580ba48db42cb937d54b212dde1d06573681bea355b305ea79843782dddfa316875ae054950d8822d8b0cbbe39e0
coinbase_p2p_private_key.operator.teratestnet.staging.2 = ebf5d29d076b3f9dbaab80b9535f1dfd7dfab579fdbed36cf2944e3073575dc29636f9d285dd5738b3b21eb5e49f19b4128fe888f38fd63d3a51abac47bb5a65
coinbase_p2p_private_key.operator.teratestnet.staging.3 = 953a0f178eda5847362eab86b90b794d720c261cf1218f1e2015e9af62f336d05026757c0e9c52821380561053e65e3b325886adbe7a2fb4e1a61c70dffa7db7

coinbase_p2p_static_peers.dev                          = /ip4/127.0.0.1/tcp/${P2P_PORT}/p2p/12D3KooWMQira6uh4rptNzMP5sojTdNXyveAWMKJi5ySoepVXGxo
coinbase_p2p_static_peers.dev.system.test              =
coinbase_p2p_static_peers.docker                       = /dns/ubsv1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/ubsv2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/ubsv3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
coinbase_p2p_static_peers.docker.m                     =
coinbase_p2p_static_peers.docker.host                  = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
coinbase_p2p_static_peers.operator.scaling             = ${M1_P2P} | ${M2_P2P} | ${M3_P2P} | ${M4_P2P} | ${M5_P2P} | ${M6_P2P}
coinbase_p2p_static_peers.operator.teratestnet.prod    = ${PROD_T1_P2P} | ${PROD_T2_P2P} | ${PROD_T3_P2P}
coinbase_p2p_static_peers.operator.teratestnet.staging = ${STAGING_T1_P2P} | ${STAGING_T2_P2P} | ${STAGING_T3_P2P}

coinbase_should_wait             = false
coinbase_should_wait.dev.simon   = false
coinbase_should_wait.docker      = false
coinbase_should_wait.docker.host = false

coinbase_store                                   = sqlite:///coinbase
coinbase_store.dev.stu                           = postgres://ubsv:ubsv@localhost:5432/coinbase
coinbase_store.dev.simon                         = postgres://ubsv:ubsv@localhost:5432/coinbase
coinbase_store.dev.liam                          = postgres://ubsv:ubsv@localhost:5432/coinbase
coinbase_store.docker                            = postgres://${coinbaseDBUserPwd}:${coinbaseDBUserPwd}@postgres:5432/${coinbaseDB}
coinbase_store.docker.ci.externaltxblaster.ubsv1 = postgres://coinbase1:coinbase1@localhost:5432/coinbase1
coinbase_store.docker.ci.externaltxblaster.ubsv2 = postgres://coinbase2:coinbase2@localhost:5432/coinbase2
coinbase_store.docker.ci.externaltxblaster.ubsv3 = postgres://coinbase3:coinbase3@localhost:5432/coinbase3
coinbase_store.docker.ci.chainintegrity.ubsv1    = postgres://coinbase1:coinbase1@localhost:5432/coinbase1
coinbase_store.docker.ci.chainintegrity.ubsv2    = postgres://coinbase2:coinbase2@localhost:5432/coinbase2
coinbase_store.docker.ci.chainintegrity.ubsv3    = postgres://coinbase3:coinbase3@localhost:5432/coinbase3
coinbase_store.docker.host                       = postgres://coinbase${PORT_PREFIX}:coinbase${PORT_PREFIX}@localhost:15432/coinbase${PORT_PREFIX}
coinbase_store.docker.m                          = postgres://coinbase1:coinbase1@postgres:5432/coinbase1









coinbase_store.host.ss.ubsv1                     = postgres://coinbase1:coinbase1@postgres:5432/coinbase1

coinbase_store_dbTimeoutMillis = 5000

# coinbase are shared on the same server
coinbase_wait_for_peers                 = true
coinbase_wait_for_peers.dev.system.test = false
coinbase_wait_for_peers.dev.kafka       = false
coinbase_wait_for_peers.operator        = false

coinbase_wallet_private_key                                 = ${PK1}
coinbase_wallet_private_key.docker.ubsv1                    = ${PK1}
coinbase_wallet_private_key.docker.ubsv2                    = ${PK2}
coinbase_wallet_private_key.docker.ubsv3                    = ${PK3}
coinbase_wallet_private_key.operator.scaling.m1             = ${PK1}
coinbase_wallet_private_key.operator.scaling.m2             = ${PK2}
coinbase_wallet_private_key.operator.scaling.m3             = ${PK3}
coinbase_wallet_private_key.operator.scaling.m4             = ${PK4}
coinbase_wallet_private_key.operator.scaling.m5             = ${PK5}
coinbase_wallet_private_key.operator.scaling.m6             = ${PK6}
coinbase_wallet_private_key.operator.teratestnet.prod.1     = ${PK1}
coinbase_wallet_private_key.operator.teratestnet.prod.2     = ${PK2}
coinbase_wallet_private_key.operator.teratestnet.prod.3     = ${PK3}
coinbase_wallet_private_key.operator.teratestnet.staging.1  = ${PK4}
coinbase_wallet_private_key.operator.teratestnet.staging.2  = ${PK5}
coinbase_wallet_private_key.operator.teratestnet.staging.3  = ${PK6}
coinbase_wallet_private_key.docker.ss.ubsv1                 = ${PK}

# time to wait in ms between sending transactions
distributer_wait_time = 10

# distributor config
distributor_backoff_duration          = 1s
distributor_backoff_duration.operator = 10s

distributor_failure_tolerance = 0

distributor_max_retries = 3

double_spend_window_millis = 0

# policy settings
# use these if you do not want unbounded scaling
excessiveblocksize                  = 4294967296
excessiveblocksize.docker.ubsv2.tc2 = 1000

# end of policy settings
faucet_httpListenAddress             = :${FAUCET_HTTP_PORT}
faucet_httpListenAddress.dev         = localhost:${FAUCET_HTTP_PORT}
faucet_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${FAUCET_HTTP_PORT}
faucet_httpListenAddress.docker.m    = :${FAUCET_HTTP_PORT}

fsm_state_restore = false
fsm_state_delay = 0
fsm_state_change_delay.docker.ubsv1 = 1s # for testing, we want to delay the state change and have time to capture the state
fsm_state_change_delay.docker.ubsv2 = 1s # for testing, we want to delay the state change and have time to capture the state
fsm_state_change_delay.docker.ubsv3 = 1s # for testing, we want to delay the state change and have time to capture the state

# @group: gocore
# Core Stats Configuration
# ------------------------
gocore_stats_reported_time_threshold = 24h

stats_prefix = gocore
# @endgroup

# @group: dashboard
# Dashboard UI
# ------------------------
dashboard_enabled = true
# @endgroup

# Grpc Resolver Configuration
grpc_resolver          = dns
grpc_resolver.operator = k8s

health_check_port             = ${HEALTH_CHECK_PORT}
health_check_port.docker.host = ${PORT_PREFIX}${HEALTH_CHECK_PORT}

http_sign_response = true

http_timeout = 1000

initial_merkle_items_per_subtree                                = 1048576
initial_merkle_items_per_subtree.dev                            = 32768
initial_merkle_items_per_subtree.docker                         = 1024
initial_merkle_items_per_subtree.docker.ubsv1.test.tna1Test          = 32
initial_merkle_items_per_subtree.docker.ubsv2.test.tna1Test        = 32
initial_merkle_items_per_subtree.docker.ubsv3.test.tna1Test         = 32
initial_merkle_items_per_subtree.docker.ubsv1.test.tnc1Test        = 32
initial_merkle_items_per_subtree.docker.ubsv2.test.tnc1Test            = 64
initial_merkle_items_per_subtree.docker.ubsv3.test.tnc1Test            = 128
initial_merkle_items_per_subtree.docker.ubsv1.test.tnj1Test          = 16
initial_merkle_items_per_subtree.docker.ubsv1.test.tnb1Test           = 32768
initial_merkle_items_per_subtree.docker.ubsv2.test.tnb1Test          = 32768
initial_merkle_items_per_subtree.docker.ubsv3.test.tnb1Test         = 32768
initial_merkle_items_per_subtree.operator.teratestnet           = 32768

# IPV6 Addresses
# --------------
# ipv6_addresses = ff02::1234

k8s_resolver_ttl     = 10
k8s_resolver_ttl.dev = 0

kafka_blocksConfig         = kafka://${KAFKA_HOSTS}/${KAFKA_BLOCKS}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&flush_bytes=64
kafka_blocksConfig.mainnet =

kafka_blocksFinalConfig         = kafka://${KAFKA_HOSTS}/${KAFKA_BLOCKS_FINAL}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&flush_bytes=64
kafka_blocksFinalConfig.mainnet =

kafka_legacyInvConfig                         = kafka://${KAFKA_HOSTS}/${KAFKA_LEGACY_INV}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=6000&flush_bytes=1024&flush_messages=16&flush_frequency=1s&consumer_ratio=4
kafka_legacyInvConfig.mainnet                 =
kafka_legacyInvConfig.operator.mainnet.euw1-1 =

kafka_rejectedTxConfig = kafka://${KAFKA_HOSTS}/${KAFKA_REJECTEDTX}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s&consumer_ratio=4

kafka_subtreesConfig         = kafka://${KAFKA_HOSTS}/${KAFKA_SUBTREES}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&segment_bytes=33554432&flush_bytes=64&flush_messages=1&consumer_ratio=1
kafka_subtreesConfig.mainnet =

kafka_txmetaConfig         = kafka://${KAFKA_HOSTS}/${KAFKA_TXMETA}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s&consumer_ratio=4
kafka_txmetaConfig.mainnet =

kafka_unitTest = kafka://localhost:${KAFKA_PORT}/${KAFKA_UNITTEST}?partitions=${KAFKA_PARTITIONS_LOW}&replication=${KAFKA_REPLICATION_FACTOR}&retention=600000&flush_bytes=1024&flush_messages=10000&flush_frequency=1s&replay=1

# tx validation order is critical so we cannot have mutiple partitions :(
kafka_validatortxsConfig                      =
kafka_validatortxsConfig.operator             = kafka://${KAFKA_HOSTS}/${KAFKA_VALIDATORTXS}?partitions=${KAFKA_PARTITIONS_HIGH}&replication=${KAFKA_REPLICATION_FACTOR}&retention=60000&flush_bytes=8192&flush_messages=10000&flush_frequency=1s
kafka_validatortxsConfig.operator.teratestnet =

# Any config from the legacy Config struct can be set here
# they need to be defined with a default value so they can be overridden from the env
legacy_config_DataDir            = ./data/legacy
legacy_config_DataDir.operator   = /data/${clientName}/legacy
# this forces IPv4 only, set to empty for IPv4 & IPv6 support
legacy_config_Listeners          = "0.0.0.0"
legacy_config_MaxPeers           = 20
legacy_config_MaxPeersPerIP      = 5
legacy_config_DisableCheckpoints = false
legacy_config_Upnp               = false
legacy_config_Upnp.dev           = true
legacy_config_Upnp.dev.simon     = false
legacy_config_Upnp.docker        = true

legacy_connect_peers                         =
legacy_connect_peers.dev.liam                = 78.110.160.26:8333 | 13.231.149.50:8333 | 54.249.171.1:8333 | 3.68.157.171:37890
legacy_connect_peers.dev.simon               =
legacy_connect_peers.mainnet                 = 10.90.0.61:8333
legacy_connect_peers.mainnet2                = 10.90.0.61:8333 | 18.199.12.185:8333
legacy_connect_peers.docker                  = 10.90.0.61:8333 | 18.199.12.185:8333
# legacy_connect_peers.docker.host           = localhost:8333
legacy_connect_peers.operator.mainnet.euw1-2 = 10.90.0.61:8333 | 18.199.12.185:8333
legacy_connect_peers.operator.mainnet.euw1-3 = 10.90.0.61:8333 | 18.199.12.185:8333
legacy_connect_peers.dev.legacy              =
legacy_connect_peers.dev.legacy.testnet      = 3.123.101.88:18333 | 3.79.205.203:18333

legacy_grpcAddress             = localhost:${LEGACY_GRPC_PORT}
legacy_grpcAddress.dev         = localhost:${LEGACY_GRPC_PORT}
legacy_grpcAddress.docker.host = localhost:${PORT_PREFIX}${LEGACY_GRPC_PORT}

legacy_grpcListenAddress             = :${LEGACY_GRPC_PORT}
legacy_grpcListenAddress.dev         = localhost:${LEGACY_GRPC_PORT}
legacy_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${LEGACY_GRPC_PORT}

legacy_httpAddress                 = http://localhost:${LEGACY_HTTP_PORT}
legacy_httpAddress.docker          = http://${clientName}:${LEGACY_HTTP_PORT}
legacy_httpAddress.docker.host     = http://localhost:${PORT_PREFIX}${LEGACY_HTTP_PORT}
legacy_httpAddress.docker.ss.ubsv1 = http://legacy-1:${LEGACY_HTTP_PORT}
legacy_httpAddress.operator        = http://legacy.${clientName}.svc.cluster.local:${LEGACY_HTTP_PORT}

legacy_httpListenAddress             = :${LEGACY_HTTP_PORT}
legacy_httpListenAddress.dev         = localhost:${LEGACY_HTTP_PORT}
legacy_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${LEGACY_HTTP_PORT}

legacy_limitedBlockValidation            = true
legacy_limitedBlockValidation.dev.legacy = false

legacy_workingDir = data/legacy

local_test_start_from_state =

# @group: logger
logLevel        = INFO
logLevel.dev    = DEBUG
logLevel.docker = INFO

# Logging Configuration
# ---------------------
logger = zerolog

logger_output_format =  | %-25s | %-7s | %s |

logger_show_socket_info = true

logger_show_timestamps = true
# @endgroup

maxtxsigopscountspolicy = 4294967295

maxtxsizepolicy = 100000000

min_block_height_for_e2e = 100

# @group: miner_wallet_private_keys compact
PK1 = L56TgyTpDdvL3W24SMoALYotibToSCySQeo4pThLKxw6EFR6f93Q
PK2 = KyAwSjuXZNgj78w3W7mR1fVMbPFu2heaCJJkWK5Yy58NZ4xafV6k
PK3 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
PK4 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
PK5 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
PK6 = L3NVjmwg3nC7ZPrwMVF6FXiG1a1RZ89nhizmJVctGztRKLYrhtFL
# @endgroup

miner_wallet_private_keys                      = ${PK1}
miner_wallet_private_keys.operator.teratestnet = ${PK1} | ${PK2} | ${PK3}
miner_wallet_private_keys.operator.scaling     = ${PK1} | ${PK1} | ${PK1} | ${PK1} | ${PK1} | ${PK1} | ${PK1} | ${PK1} | ${PK1} | ${PK1}
miner_wallet_private_keys.docker               = ${PK1} | ${PK2} | ${PK3}

# default of 1 sat / kb
minminingtxfee             = 0.00000001
minminingtxfee.docker      = 0

# network can be  main, stn, test and regtest.
network                          = mainnet
network.dev                      = regtest # used by unit tests in your IDE
network.dev.simon                = testnet
network.dev.stu                  = regtest
network.test                     = regtest # used by long tests
network.docker                   = regtest
network.docker.ubsv1.test.tnf6   = testnet
network.docker.ubsv2.test.tnf6   = custom
network.docker.ubsv3.test.tnf6   = testnet
network.docker.ubsv2.test.tnf6.stage2 = testnet
network.dev.liam                 = regtest
network.operator                 = mainnet
network.operator.teratestnet     = testnet
network.dev.legacy.testnet       = testnet
network.dev.legacy.mainnet       = mainnet

p2p_bestblock_topic = bestblock

p2p_block_topic = block

p2p_bootstrapAddresses          = /dns4/eu-p2pbootstrap.ubsv.dev/tcp/${P2P_BOOTSTRAP_PORT}/p2p/12D3KooWESmhNAN8s6NPdGNvJH3zJ4wMKDxapXKNUe2DzkAwKYqK | /dns4/us-p2pbootstrap.ubsv.dev/tcp/${P2P_BOOTSTRAP_PORT}/p2p/12D3KooWJ6kQHAR65xkA34NABsNVAJyVxPWh8JUSo1vtZsTyw4GD
p2p_bootstrapAddresses.operator = /dns4/p2pbootstrap-svc-euc.p2pbootstrap.svc.cluster.local/tcp/${P2P_BOOTSTRAP_PORT}/p2p/12D3KooWKF5qzT7TkB6KMXehKauvaqaTLcXoQRZtSvhavmfTifu4

p2p_dht_protocol_id = /ubsv

p2p_dht_use_private          = false
p2p_dht_use_private.dev      = true
p2p_dht_use_private.docker   = true
p2p_dht_use_private.operator = true

p2p_grpcAddress             = localhost:${P2P_GRPC_PORT}
p2p_grpcAddress.docker.host = localhost:${PORT_PREFIX}${P2P_GRPC_PORT}

p2p_grpcListenAddress             = :${P2P_GRPC_PORT}
p2p_grpcListenAddress.dev         = localhost:${P2P_GRPC_PORT}
p2p_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${P2P_GRPC_PORT}

p2p_httpAddress                   = localhost:${P2P_HTTP_PORT}
p2p_httpAddress.dev               = localhost:${P2P_HTTP_PORT}
p2p_httpAddress.operator          = peer.${clientName}.svc.cluster.local:${P2P_HTTP_PORT}
p2p_httpAddress.docker.m          = ubsv-p2p:${P2P_HTTP_PORT}
p2p_httpAddress.docker.host       = localhost:${PORT_PREFIX}${P2P_HTTP_PORT}
p2p_httpAddress.docker.ss.ubsv1   = p2p-1:${P2P_HTTP_PORT}
p2p_httpAddress.docker.ubsv1.tec5 =

p2p_httpListenAddress                   = :${P2P_HTTP_PORT}
p2p_httpListenAddress.dev               = localhost:${P2P_HTTP_PORT}
p2p_httpListenAddress.docker            = localhost:${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.host       = localhost:${PORT_PREFIX}${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.m          = :${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.ss         = 0.0.0.0:${P2P_HTTP_PORT}
p2p_httpListenAddress.docker.ubsv1.tec5 =

p2p_ip     = 0.0.0.0
p2p_ip.dev = 127.0.0.1

p2p_mining_on_topic = miningon

# dev env runs in same process so must have different port
p2p_optimise_retries = false

# @group: p2p_peer_id compact
OPERATOR1_P2P_ID = 12D3KooWFmtrxJsWUQo3axbyUAqXMH33G9HXmd3xaW3XuhsKccLn
OPERATOR2_P2P_ID = 12D3KooWSPjxch3frXkbq612xz7Xbs7y1w47hvLieDt7r3mRE43q
OPERATOR3_P2P_ID = 12D3KooWRxN9gLTfVZXLGBYLrijar3bkstXWpo2HMY23JLFpXDfj
OPERATOR4_P2P_ID = 12D3KooWN6Ck8yddcRCuZa6B2Ti5noUsv6Tb5Q2LVk9KQ6nF46oJ
OPERATOR5_P2P_ID = 12D3KooWPdTFCqbUWEddkpuwToFAXCpU5ZhNYn8n6tSXbWqAK7G4
OPERATOR6_P2P_ID = 12D3KooWHGfHqZ2PgBmcSxwdWGjtS8E6Yvy9nzMpFbnX1Attfmws
# @endgroup

p2p_peer_id.dev                            = 12D3KooWMQira6uh4rptNzMP5sojTdNXyveAWMKJi5ySoepVXGxo
p2p_peer_id.docker.ubsv1                   = 12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG
p2p_peer_id.docker.ubsv2                   = 12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_peer_id.docker.ubsv3                   = 12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_peer_id.docker.host.ubsv1              = 12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG
p2p_peer_id.docker.host.ubsv2              = 12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_peer_id.docker.host.ubsv3              = 12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_peer_id.docker.ss.ubsv1                = 12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG
p2p_peer_id.operator.scaling.m1            = ${OPERATOR1_P2P_ID}
p2p_peer_id.operator.scaling.m2            = ${OPERATOR2_P2P_ID}
p2p_peer_id.operator.scaling.m3            = ${OPERATOR3_P2P_ID}
p2p_peer_id.operator.scaling.m4            = ${OPERATOR4_P2P_ID}
p2p_peer_id.operator.scaling.m5            = ${OPERATOR5_P2P_ID}
p2p_peer_id.operator.scaling.m6            = ${OPERATOR6_P2P_ID}
p2p_peer_id.operator.teratestnet.prod.1    = ${OPERATOR1_P2P_ID}
p2p_peer_id.operator.teratestnet.prod.2    = ${OPERATOR2_P2P_ID}
p2p_peer_id.operator.teratestnet.prod.3    = ${OPERATOR3_P2P_ID}
p2p_peer_id.operator.teratestnet.staging.1 = ${OPERATOR4_P2P_ID}
p2p_peer_id.operator.teratestnet.staging.2 = ${OPERATOR5_P2P_ID}
p2p_peer_id.operator.teratestnet.staging.3 = ${OPERATOR6_P2P_ID}

p2p_port             = ${P2P_PORT}
p2p_port.docker.host = ${PORT_PREFIX}${P2P_PORT}

p2p_port_coinbase             = ${P2P_PORT}
p2p_port_coinbase.dev         = ${P2P_PORT_COINBASE}
p2p_port_coinbase.docker      = ${P2P_PORT_COINBASE}
p2p_port_coinbase.docker.host = ${PORT_PREFIX}${P2P_PORT_COINBASE}

# create your own private key and peer ID using cmd/keygen
p2p_private_key.dev                            = 90de404ee469fb284feb6ba93b429b95d7e0eb1c5ff530dc06a50cf268c72683ac3db1b6c7527b9b853e72647e760798125441f7490353a01a6b7e056df843d0
p2p_private_key.docker.ubsv1                   = c8a1b91ae120878d91a04c904e0d565aa44b2575c1bb30a729bd3e36e2a1d5e6067216fa92b1a1a7e30d0aaabe288e25f1efc0830f309152638b61d84be6b71d
p2p_private_key.docker.ubsv2                   = 89a2d8acf5b2e60fd969914c326c63cde50675a47897c0eaacc02eb6ff8665585d4d059f977910472bcb75040617632019cc0749443fdc66d331b61c8cfb4b0f
p2p_private_key.docker.ubsv3                   = d77a7cac7833f2c0263ed7b9aaeb8dda1effaf8af948d570ed8f7a93bd3c418d6efee7bdd82ddb80484be84ba0c78ea07251a3ba2b45b2b3367fd5e2f0284e7c
p2p_private_key.docker.host.ubsv1              = c8a1b91ae120878d91a04c904e0d565aa44b2575c1bb30a729bd3e36e2a1d5e6067216fa92b1a1a7e30d0aaabe288e25f1efc0830f309152638b61d84be6b71d
p2p_private_key.docker.host.ubsv2              = 89a2d8acf5b2e60fd969914c326c63cde50675a47897c0eaacc02eb6ff8665585d4d059f977910472bcb75040617632019cc0749443fdc66d331b61c8cfb4b0f
p2p_private_key.docker.host.ubsv3              = d77a7cac7833f2c0263ed7b9aaeb8dda1effaf8af948d570ed8f7a93bd3c418d6efee7bdd82ddb80484be84ba0c78ea07251a3ba2b45b2b3367fd5e2f0284e7c
p2p_private_key.docker.ss.ubsv1                = c8a1b91ae120878d91a04c904e0d565aa44b2575c1bb30a729bd3e36e2a1d5e6067216fa92b1a1a7e30d0aaabe288e25f1efc0830f309152638b61d84be6b71d
p2p_private_key.operator.scaling.m1            = 97f0144e36f55a9aa1a8e4f8a49a417c9552fc3a224dcd785e1704cab714290c5884158964322bf79698dce9f8089f7e2e78d10afe597d7fe18e0b4328dd823f
p2p_private_key.operator.scaling.m2            = bc68e5570105978ef622544be8c02e00f773a8248955be03ac65b0b4fde48ed8f647cbeb6ac81a3e996231e45b16ee413838f633f90c61fcb7b44f2b15b17818
p2p_private_key.operator.scaling.m3            = 4152ea22c2152ae4ae662809331cf4e05efecef970a000961016cd7a7cbcda52efc79dd47f78dc8de72ee9d055156a1d7a9b4c84d90fe61398fb3395e2b52d16
p2p_private_key.operator.scaling.m4            = fe7e0ae1c9a13ddc1e72b2b478e099eca87c1782f1e2af2b662c9b1cb1db8f25b65ae14a0387479658f5034cd0f2c000e2b1cecb98d899fc15570ee29b30cd69
p2p_private_key.operator.scaling.m5            = 6febeeceae316d0c7330f1ff47108391cef23e28d9cb09aa74d2d93c449be688cd37f7f276c68c3eb6063d2f75083c9b467f1a0407196cb7779bf092073032e1
p2p_private_key.operator.scaling.m6            = 5e08423f6db13b5150614d161f0ccda802fb6373b1193d7d2f405a7e49e9c7496ebe450f86db96eae79cb14cd739d15e357b04bd7a30a60dd154086d59ee69de
p2p_private_key.operator.teratestnet.prod.1    = 97f0144e36f55a9aa1a8e4f8a49a417c9552fc3a224dcd785e1704cab714290c5884158964322bf79698dce9f8089f7e2e78d10afe597d7fe18e0b4328dd823f
p2p_private_key.operator.teratestnet.prod.2    = bc68e5570105978ef622544be8c02e00f773a8248955be03ac65b0b4fde48ed8f647cbeb6ac81a3e996231e45b16ee413838f633f90c61fcb7b44f2b15b17818
p2p_private_key.operator.teratestnet.prod.3    = 4152ea22c2152ae4ae662809331cf4e05efecef970a000961016cd7a7cbcda52efc79dd47f78dc8de72ee9d055156a1d7a9b4c84d90fe61398fb3395e2b52d16
p2p_private_key.operator.teratestnet.staging.1 = fe7e0ae1c9a13ddc1e72b2b478e099eca87c1782f1e2af2b662c9b1cb1db8f25b65ae14a0387479658f5034cd0f2c000e2b1cecb98d899fc15570ee29b30cd69
p2p_private_key.operator.teratestnet.staging.2 = 6febeeceae316d0c7330f1ff47108391cef23e28d9cb09aa74d2d93c449be688cd37f7f276c68c3eb6063d2f75083c9b467f1a0407196cb7779bf092073032e1
p2p_private_key.operator.teratestnet.staging.3 = 5e08423f6db13b5150614d161f0ccda802fb6373b1193d7d2f405a7e49e9c7496ebe450f86db96eae79cb14cd739d15e357b04bd7a30a60dd154086d59ee69de

# The P2P topic to publish rejected transaction messages
p2p_rejected_tx_topic = rejected_tx

p2p_shared_key = 285b49e6d910726a70f205086c39cbac6d8dcc47839053a21b1f614773bbc137

# @group: p2p_static_peers compact
M1_P2P         = /dns/m1.scaling.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR1_P2P_ID}
M2_P2P         = /dns/m2.scaling.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR2_P2P_ID}
M3_P2P         = /dns/m3.scaling.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR3_P2P_ID}
M4_P2P         = /dns/m4.scaling.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR4_P2P_ID}
M5_P2P         = /dns/m5.scaling.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR5_P2P_ID}
M6_P2P         = /dns/m6.scaling.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR6_P2P_ID}
PROD_T1_P2P    = /dns/prod-teranet-1.teratestnet.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR1_P2P_ID}
PROD_T2_P2P    = /dns/prod-teranet-2.teratestnet.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR2_P2P_ID}
PROD_T3_P2P    = /dns/prod-teranet-3.teratestnet.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR3_P2P_ID}
STAGING_T1_P2P = /dns/staging-teranet-1.teratestnet.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR4_P2P_ID}
STAGING_T2_P2P = /dns/staging-teranet-2.teratestnet.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR5_P2P_ID}
STAGING_T3_P2P = /dns/staging-teranet-3.teratestnet.ubsv.dev/tcp/${P2P_PORT}/p2p/${OPERATOR6_P2P_ID}
# @endgroup

# p2p_static_peers is optional, the node will use the bootstrap addresses to find peers regardless
p2p_static_peers                                =
p2p_static_peers.docker.ubsv1                   = /dns/ubsv2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/ubsv3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.ubsv2                   = /dns/ubsv1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/ubsv3/tcp/${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.ubsv3                   = /dns/ubsv1/tcp/${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/ubsv2/tcp/${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_static_peers.docker.m                       =
p2p_static_peers.docker.host.ubsv1              = /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.host.ubsv2              = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/3${P2P_PORT}/p2p/12D3KooWHHeTM3aK4s9DKS6DQ7SbBb7czNyJsPZtQiUKa4fduMB9
p2p_static_peers.docker.host.ubsv3              = /dns/localhost/tcp/1${P2P_PORT}/p2p/12D3KooWAFXWuxgdJoRsaA4J4RRRr8yu6WCrAPf8FaS7UfZg3ceG | /dns/localhost/tcp/2${P2P_PORT}/p2p/12D3KooWG6aCkDmi5tqx4G4AvVDTQdSVvTSzzQvk1vh9CtSR8KEW
p2p_static_peers.operator.scaling.m1            = ${M2_P2P} | ${M3_P2P} | ${M4_P2P} | ${M5_P2P} | ${M6_P2P}
p2p_static_peers.operator.scaling.m2            = ${M1_P2P} | ${M3_P2P} | ${M4_P2P} | ${M5_P2P} | ${M6_P2P}
p2p_static_peers.operator.scaling.m3            = ${M1_P2P} | ${M2_P2P} | ${M4_P2P} | ${M5_P2P} | ${M6_P2P}
p2p_static_peers.operator.scaling.m4            = ${M1_P2P} | ${M2_P2P} | ${M3_P2P} | ${M5_P2P} | ${M6_P2P}
p2p_static_peers.operator.scaling.m5            = ${M1_P2P} | ${M2_P2P} | ${M3_P2P} | ${M4_P2P} | ${M6_P2P}
p2p_static_peers.operator.scaling.m6            = ${M1_P2P} | ${M2_P2P} | ${M3_P2P} | ${M4_P2P} | ${M5_P2P}
p2p_static_peers.operator.teratestnet.prod.1    = ${PROD_T2_P2P} | ${PROD_T3_P2P}
p2p_static_peers.operator.teratestnet.prod.2    = ${PROD_T1_P2P} | ${PROD_T3_P2P}
p2p_static_peers.operator.teratestnet.prod.3    = ${PROD_T1_P2P} | ${PROD_T2_P2P}
p2p_static_peers.operator.teratestnet.staging.1 = ${STAGING_T2_P2P} | ${STAGING_T3_P2P}
p2p_static_peers.operator.teratestnet.staging.2 = ${STAGING_T1_P2P} | ${STAGING_T3_P2P}
p2p_static_peers.operator.teratestnet.staging.3 = ${STAGING_T1_P2P} | ${STAGING_T2_P2P}

p2p_subtree_topic = subtree

p2p_topic_prefix                      = github.com/bitcoin-sv/ubsv
p2p_topic_prefix.dev                  = dev.github.com/bitcoin-sv/ubsv
p2p_topic_prefix.dev.simon            = dev.simon.github.com/bitcoin-sv/ubsv
p2p_topic_prefix.operator.scaling     = scaling.github.com/bitcoin-sv/ubsv
p2p_topic_prefix.operator.teratestnet = testing.github.com/bitcoin-sv/ubsv

peerStatus_timeout = 5m

# Profiler Configuration
# ---------------------------------------
profilerAddr             = :${PROFILE_PORT}
profilerAddr.dev         = localhost:${PROFILE_PORT}
profilerAddr.docker.host = localhost:${PORT_PREFIX}${PROFILE_PORT}

# Prometheus and Tracing Configuration
# ---------------------------------------
prometheusEndpoint = /metrics

propagation_grpcAddress.docker   = ${clientName}:${PROPAGATION_GRPC_PORT}
propagation_grpcAddress.docker.m = ubsv-propagation:${PROPAGATION_GRPC_PORT}

# @group: propagation_grpcAddresses compact
M1         = dns:///m1.scaling.ubsv.dev:${PROPAGATION_GRPC_PORT}
M2         = dns:///m2.scaling.ubsv.dev:${PROPAGATION_GRPC_PORT}
M3         = dns:///m3.scaling.ubsv.dev:${PROPAGATION_GRPC_PORT}
M4         = dns:///m4.scaling.ubsv.dev:${PROPAGATION_GRPC_PORT}
M5         = dns:///m5.scaling.ubsv.dev:${PROPAGATION_GRPC_PORT}
M6         = dns:///m6.scaling.ubsv.dev:${PROPAGATION_GRPC_PORT}
PROD_T1    = dns:///prod-teranet-1.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
PROD_T2    = dns:///prod-teranet-2.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
PROD_T3    = dns:///prod-teranet-3.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
STAGING_T1 = dns:///staging-teranet-1.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
STAGING_T2 = dns:///staging-teranet-2.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
STAGING_T3 = dns:///staging-teranet-3.teratestnet.ubsv.dev:${PROPAGATION_GRPC_PORT}
# @endgroup

# Note the following settings can be a pipe separated list
propagation_grpcAddresses                              = localhost:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker                       = ubsv1:${PROPAGATION_GRPC_PORT} | ubsv2:${PROPAGATION_GRPC_PORT} | ubsv3:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.host                  = localhost:1${PROPAGATION_GRPC_PORT} | localhost:2${PROPAGATION_GRPC_PORT} | localhost:3${PROPAGATION_GRPC_PORT}
# propagation_grpcAddresses.docker.host                = localhost:1${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.m                     = ubsv-propagation:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.docker.ss.ubsv1              = propagation-1:${PROPAGATION_GRPC_PORT}
propagation_grpcAddresses.operator.scaling             = ${M1} | ${M2} | ${M3} | ${M4} | ${M5} | ${M6}
propagation_grpcAddresses.operator.teratestnet.prod    = ${PROD_T1} | ${PROD_T2} | ${PROD_T3}
propagation_grpcAddresses.operator.teratestnet.staging = ${STAGING_T1} | ${STAGING_T2} | ${STAGING_T3}

propagation_grpcListenAddress             = :${PROPAGATION_GRPC_PORT}
propagation_grpcListenAddress.dev         = localhost:${PROPAGATION_GRPC_PORT}
propagation_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${PROPAGATION_GRPC_PORT}

propagation_grpcMaxConnectionAge          = 30s
propagation_grpcMaxConnectionAge.operator = 5m

propagation_httpAddresses                 = http://localhost:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker          = http://${clientName}:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker.host     = http://localhost:${PORT_PREFIX}${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker.m        = http://ubsv-propagation:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.operator        = http://propagation.${clientName}.svc.cluster.local:${PROPAGATION_HTTP_PORT}
propagation_httpAddresses.docker.ss.ubsv1 = http://propagation-1:${PROPAGATION_HTTP_PORT}

# 6 nodes
propagation_httpListenAddress             = :${PROPAGATION_HTTP_PORT}
propagation_httpListenAddress.dev         = localhost:${PROPAGATION_HTTP_PORT}
propagation_httpListenAddress.docker.host = localhost:${PORT_PREFIX}${PROPAGATION_HTTP_PORT}

# Note the following settings can be a pipe separated list
propagation_quicAddresses          = https://localhost:${PROPAGATION_QUIC_PORT}
propagation_quicAddresses.docker   =
propagation_quicAddresses.operator =

propagation_quicListenAddress             =
propagation_quicListenAddress.dev         = localhost:${PROPAGATION_QUIC_PORT}
propagation_quicListenAddress.docker.host = localhost:${PORT_PREFIX}${PROPAGATION_QUIC_PORT}

# set this to 0 to disable the propagation client from sending transactions to the propagation service in batches
propagation_sendBatchSize = 100

propagation_sendBatchTimeout = 10

# The alert service uses this to connect to the rpc
rpc_address = http://localhost:${RPC_PORT}

rpc_listener_url                   = :${RPC_PORT}
rpc_listener_url.docker.host.ubsv1 = :1${RPC_PORT}
rpc_listener_url.docker.host.ubsv2 = :2${RPC_PORT}
rpc_listener_url.docker.host.ubsv3 = :3${RPC_PORT}

rpc_max_clients = 3

rpc_pass = bitcoin

rpc_user = bitcoin

securityLevelGRPC = 0

# Security Configuration
# ---------------------------------------
securityLevelHTTP = 0

server_certFile = certs/ubsv.crt

server_keyFile = certs/ubsv.key

# how long to wait before deleting spent utxos (seconds)
spent_utxo_ttl        = 3600 # = 1 hour
spent_utxo_ttl.dev    = 10 # = 10 seconds
spent_utxo_ttl.docker = 10 # = 10 seconds

# Alert Service Configuration
# ---------------------------------------
startAlert                 = true
startAlert.dev.stu         = false
startAlert.dev.system.test = false
startAlert.docker.host     = false
startAlert.docker.m        = false
startAlert.operator        = false

# Asset Service Configuration
# ---------------------------------------
startAsset                                  = true
startAsset.docker.m                         = false
startAsset.operator                         = false
startAsset.docker.ubsv2.test.resilience.tc6 = false

# Block Assembly Service Configuration
# ---------------------------------------
startBlockAssembly                                  = true
startBlockAssembly.dev.system.test.blockassembly    = false
startBlockAssembly.docker.m                         = false
startBlockAssembly.operator                         = false
startBlockAssembly.docker.ubsv2.test.resilience.tc2 = false

# Block Persister Service Configuration
# ---------------------------------------
startBlockPersister                                = false
startBlockPersister.docker                         = true
startBlockPersister.docker.m                       = false
startBlockPersister.m                              = false
startBlockPersister.dev                            = true
startBlockPersister.operator                       = false

# Block Validation Service Configuration
# ---------------------------------------
startBlockValidation                                  = true
startBlockValidation.docker.m                         = false
startBlockValidation.operator                         = false
startBlockValidation.docker.ubsv2.test.resilience.tc3 = false

# BlockChain Service Configuration
# ---------------------------------------
startBlockchain                                  = true
startBlockchain.docker.ubsv1.test.stopBlockchain = false
startBlockchain.docker.m                         = false
startBlockchain.operator                         = false
startBlockchain.docker.ubsv2.test.resilience.tc4 = false

# Coinbase Tracker Service Configuration
# ---------------------------------------
startCoinbase                                = true
startCoinbase.dev.system.test.stopcoinbase   = false
startCoinbase.docker                         = true
startCoinbase.docker.m                       = false
startCoinbase.operator                       = false
startCoinbase.docker.ubsv1.tc1               = false
startCoinbase.docker.ubsv2.tc1               = false
startCoinbase.docker.ubsv3.tc1               = false
startCoinbase.docker.host.ubsv1              = true
startCoinbase.docker.host.ubsv2              = false
startCoinbase.docker.host.ubsv3              = false

# Faucet Service Configuration
# ---------------------------------------
startFaucet          = true
startFaucet.operator = false
startFaucet.docker   = false
startFaucet.docker.m = false

# Legacy Service Configuration
# ---------------------------------------
startLegacy            = false
startLegacy.dev.liam   = true
startLegacy.dev.legacy = true

# P2P Configuration
# -----------------
startP2P                                  = true
startP2P.dev.system.test                  = false
startP2P.operator                         = false
startP2P.docker.m                         = false
startP2P.docker.ubsv2.test.stopP2P        = false
startP2P.docker.ubsv3.test.stopP2P        = false
startP2P.docker.ubsv2.tc3                 = false
startP2P.docker.ubsv2.test.tnf6.stage1    = false
startP2P.docker.ubsv2.test.tnf6.stage2    = true
startP2P.docker.ubsv2.test.resilience.tc5 = false

# Propagation Service Configuration
# ---------------------------------
startPropagation                                  = true
startPropagation.dev.system.test                  = true
startPropagation.docker.m                         = false
startPropagation.operator                         = false
startPropagation.docker.ubsv2.test.resilience.tc1 = false

# rpc service
# -----------
startRPC                 = false
startRPC.dev.stu         = true
startRPC.dev.system.test = true
startRPC.dev.liam        = true
startRPC.docker          = true

# Subtree Validation Service Configuration
# ----------------------------------------
startSubtreeValidation          = true
startSubtreeValidation.docker.m = false
startSubtreeValidation.operator = false

# UTXO Persister Service Configuration
# ------------------------------------
startUTXOPersister = false

# Validator Service Configuration
# ----------------------------------------
# startValidator is set to true to start the validator as a service
startValidator             = true
startValidator.mainnet2    = false
startValidator.docker      = true
startValidator.docker.host = true
startValidator.operator    = false
startValidator.docker.ss   = false

subtree_quorum_absolute_timeout = 30s

subtree_quorum_path             = ./data/subtree_quorum
subtree_quorum_path.docker      = ./data/subtreestore/subtree_quorum
subtree_quorum_path.docker.host = ./data/${clientName}/subtree_quorum
subtree_quorum_path.operator    = /data/${clientName}/subtreestore/quorum

subtreestore                                      = file://./data/subtreestore?localTTLStore=file&localTTLStorePath=data/subtreestore-ttl-1 | data/subtreestore-ttl-2
subtreestore.dev                                  = null:///?localTTLStore=file&localTTLStorePath=./data/subtreestore-ttl
subtreestore.dev.simon                            = file://./data/subtreestore
subtreestore.mainnet                              = file://./data/subtreestore
# subtreestore.mainnet2                           = s3:///teranode-gateway-test-eu-central-1?region=eu-central-1&subDirectory=sv-node/subtreestore
# subtreestore.mainnet2                           = lustre://s3.${regionName}.amazonaws.com/ubsv-${clientName}-block-store?region=${regionName}&localDir=data/ibd/s3/sv-node/subtreestore&localPersist=s3
subtreestore.mainnet2                             = file:///data/ibd/subtreestore?hashPrefix=0
subtreestore.docker                               = file://./data/subtreestore
subtreestore.docker.host                          = file://./data/${clientName}/subtreestore
# subtreestore.operator                           = lustre://s3.${regionName}.amazonaws.com/ubsv-${clientName}-subtree-store?region=${regionName}&localDir=/data/${clientName}/subtreestore&localPersist=s3
subtreestore.operator                             = file:///data/${clientName}/subtreestore
subtreestore.operator.mainnet.euw1-2              = file:///data/${clientName}/subtreestore?hashPrefix=0
subtreestore.operator.teratestnet                 = lustre:///?region=${regionName}&localDir=/data/${clientName}/subtreestore&localPersist=s3
subtreestore.docker.ubsv1.test.context.testrunner = file://./../../data/test/ubsv1/subtreestore
subtreestore.docker.ubsv2.test.context.testrunner = file://./../../data/test/ubsv2/subtreestore
subtreestore.docker.ubsv3.test.context.testrunner = file://./../../data/test/ubsv3/subtreestore
subtreestore.docker.ubsv1.context.testrunner      = file://./../../data/ubsv1/subtreestore
subtreestore.docker.ubsv2.context.testrunner      = file://./../../data/ubsv2/subtreestore
subtreestore.docker.ubsv3.context.testrunner      = file://./../../data/ubsv3/subtreestore

# quick validation means it only uses the txmeta cache
# (ignored if subtreevalidation_validation_max_retries is 0)
subtreevalidation_failfast_validation        = true
subtreevalidation_failfast_validation.docker = false

subtreevalidation_getMissingTransactions = 32

subtreevalidation_grpcAddress             = localhost:${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcAddress.docker.m    = ubsv-subtreevalidation:${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcAddress.docker.host = localhost:${PORT_PREFIX}${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcAddress.operator    = k8s:///subtree-validator.${clientName}.svc.cluster.local:${SUBTREE_VALIDATION_GRPC_PORT}

subtreevalidation_grpcListenAddress             = :${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcListenAddress.dev         = localhost:${SUBTREE_VALIDATION_GRPC_PORT}
subtreevalidation_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${SUBTREE_VALIDATION_GRPC_PORT}

subtreevalidation_processTxMetaUsingCache_BatchSize = 1024

subtreevalidation_processTxMetaUsingCache_Concurrency = 32

# cache miss threshold is the number of missing txs to allow before we fail
subtreevalidation_processTxMetaUsingCache_MissingTxThreshold = 1

subtreevalidation_processTxMetaUsingStore_BatchSize = 1024

subtreevalidation_processTxMetaUsingStore_Concurrency = 32

# store miss threshold is the number of missing txs when searching our store, ignored during BlockVaidation
subtreevalidation_processTxMetaUsingStore_MissingTxThreshold = 1

# subtree validation concurrency settings
# this is basically making subtree processing unlimited
# it is very important that new subtrees are processed as fast as possible
subtreevalidation_subtreeFoundChConcurrency          = 1
subtreevalidation_subtreeFoundChConcurrency.docker   = 1
subtreevalidation_subtreeFoundChConcurrency.operator = 128

# default is 120 (minutes)
subtreevalidation_subtreeTTL.docker.host = 0
#  replace with subtreevalidation_subtreeTTLDuration
subtreevalidation_subtreeTTLDuration.docker.host = 0

# limit this, this is happening in the background
subtreevalidation_subtreeTTLConcurrency = 8

subtreevalidation_subtreeValidationTimeout = 1000

# abandon subtree validation when missing total exceeds threshold, ignored during BlockVaidation
subtreevalidation_subtree_validation_abandon_threshold = 1

subtreevalidation_txMetaCacheEnabled = true

subtreevalidation_validation_max_retries = 30

subtreevalidation_validation_retry_sleep = 5s

temp_store          = file://./data/tempstore
temp_store.operator = file:///data/${clientName}/tempstore

# E2E test configuration
test_run_mode        = dev
test_run_mode.docker = ci

# testing environment is reusing the same kafka cluster for cost saving so we need to redefine the topics to differentiate them =
# testing environment is reusing the same kafka cluster for cost saving so we need to redefine the topics to differentiate them =

tracing_SampleRate = 0.1

# careful! this variable only works for tminer-lo-1he OTEL tracer. If you're using open tracing you need to set JAEGER_AGENT_HOST
tracing_collector_url             = jaeger://jaeger-cluster-agent.jaeger.svc.cluster.local:${JAEGER_PORT_HTTP}
tracing_collector_url.dev         = localhost:${JAEGER_PORT_HTTP}
tracing_collector_url.docker.host = jaeger://localhost:${JAEGER_PORT_HTTP}
tracing_collector_url.docker      = jaeger://jaeger:${JAEGER_PORT_HTTP}
tracing_collector_url.mainnet2    = jaeger://:${JAEGER_PORT_HTTP}
tracing_collector_url.operator    = jaeger://jaeger-agent.jaeger.svc.cluster.local:${JAEGER_PORT_HTTP}

# turned off for M3 for now, we do not have a Kafka there yet
# Tx Meta Data Store Service Configuration
# ----------------------------------------
txMetaCacheMaxMB                      = 256 # 512MB
txMetaCacheMaxMB.docker               = 128
txMetaCacheMaxMB.operator             = 131072 # 128GB - around 1.3B - 1.5B txs
txMetaCacheMaxMB.operator.teratestnet = 512 # 128GB - around 1.3B - 1.5B txs

txMetaCacheTrimRatio          = 5
txMetaCacheTrimRatio.operator = 5

tx_blaster_p2p_peer_id                                = 12D3KooWLRaBuYhHPcj8ixqwiVLtJ6YxMMSJRDwPYKVN5HWSnUWo
tx_blaster_p2p_peer_id.operator.scaling.m1            = 12D3KooWBNSXb2x1tUjpofMXckHwmATVGD9kYRT5reBCJEnxXVUQ
tx_blaster_p2p_peer_id.operator.scaling.m2            = 12D3KooWBjaVFuwk7S2KHKjnoHNembb6DQDkrumoG24idHKXi7Jd
tx_blaster_p2p_peer_id.operator.scaling.m3            = 12D3KooWNgvGZztjDqJQmCTH5fSap2vpfm6XKgWTg1sdyaVWm1RX
tx_blaster_p2p_peer_id.operator.scaling.m4            = 12D3KooWSfHjGiZRG9CS8q9ojTmQTfSFP2KW3bnZZjbfiX6J8MFZ
tx_blaster_p2p_peer_id.operator.scaling.m5            = 12D3KooWCNZMNSeqB6a3NqhvEqrsc6zUABEQFMvAvnsQr7UmBm2P
tx_blaster_p2p_peer_id.operator.scaling.m6            = 12D3KooWHZayp2ZRQyp4SLWr3mwnCkmqyo1HdcEYnFMZdmkHufav
tx_blaster_p2p_peer_id.operator.teratestnet.prod.1    = 12D3KooWBNSXb2x1tUjpofMXckHwmATVGD9kYRT5reBCJEnxXVUQ
tx_blaster_p2p_peer_id.operator.teratestnet.prod.2    = 12D3KooWBjaVFuwk7S2KHKjnoHNembb6DQDkrumoG24idHKXi7Jd
tx_blaster_p2p_peer_id.operator.teratestnet.prod.3    = 12D3KooWNgvGZztjDqJQmCTH5fSap2vpfm6XKgWTg1sdyaVWm1RX
tx_blaster_p2p_peer_id.operator.teratestnet.staging.1 = 12D3KooWSfHjGiZRG9CS8q9ojTmQTfSFP2KW3bnZZjbfiX6J8MFZ
tx_blaster_p2p_peer_id.operator.teratestnet.staging.2 = 12D3KooWCNZMNSeqB6a3NqhvEqrsc6zUABEQFMvAvnsQr7UmBm2P
tx_blaster_p2p_peer_id.operator.teratestnet.staging.3 = 12D3KooWHZayp2ZRQyp4SLWr3mwnCkmqyo1HdcEYnFMZdmkHufav

tx_blaster_p2p_private_key                                = f19576f6a8ea51ebf5a1fb05cdfea7b66bfa8ecdcec58232d064f3343fb19ab29d99c9e7339b479ed4ee48d7bb7a078bcc9e2ad2355e7e6ba94297e97ecb7744
tx_blaster_p2p_private_key.operator.scaling.m1            = c46672a8343179958b9bc76a0a4079e5ffbfc64aa89316d944b2316bd2e65582171333b40da75e31d23d76c231d67762a9031dcb0cbe725e2114599900b7b125
tx_blaster_p2p_private_key.operator.scaling.m2            = 74473a119f9adc5b49c0ccd9974c687974ab0ecd0f9330f4d14b3f16fb8c7be71c7d662fef5bc33de42257940a0f79f276e03e06a6f584b178b82d700f49e15e
tx_blaster_p2p_private_key.operator.scaling.m3            = f0728085429c185ce0b4bcbe48f0b912f5f87464e2ac15a5358900d3e14b5486bf3f9463df2abea9c198d3376b1066be16738f8cd8447fd303802cf82531c6fe
tx_blaster_p2p_private_key.operator.scaling.m4            = 503341a408e245743da6df4dbf2e22d1c28bfe0d56a23151da17169225e0cc32fa436e5a4a849293c667863fb876025631a87ab87041037d65941dd4866cd764
tx_blaster_p2p_private_key.operator.scaling.m5            = ce29af2fa4eb174ea6612cbc7d3bd354e7afea8622c385e84e7a68a5238273ad25f69ac977c9400fb1e4444f1b81c7e21aac680eb3f36a7278a56a5d6cd40db0
tx_blaster_p2p_private_key.operator.scaling.m6            = d0007fe900755a81635d0203831fe551175896f510f0dc2f8639bac840c4a275731444f003ecfaeed8c0fb172124bb33c7c559a84accd04b2ab2d4ef5ec7a307
tx_blaster_p2p_private_key.operator.teratestnet.prod.1    = c46672a8343179958b9bc76a0a4079e5ffbfc64aa89316d944b2316bd2e65582171333b40da75e31d23d76c231d67762a9031dcb0cbe725e2114599900b7b125
tx_blaster_p2p_private_key.operator.teratestnet.prod.2    = 74473a119f9adc5b49c0ccd9974c687974ab0ecd0f9330f4d14b3f16fb8c7be71c7d662fef5bc33de42257940a0f79f276e03e06a6f584b178b82d700f49e15e
tx_blaster_p2p_private_key.operator.teratestnet.prod.3    = f0728085429c185ce0b4bcbe48f0b912f5f87464e2ac15a5358900d3e14b5486bf3f9463df2abea9c198d3376b1066be16738f8cd8447fd303802cf82531c6fe
tx_blaster_p2p_private_key.operator.teratestnet.staging.1 = 503341a408e245743da6df4dbf2e22d1c28bfe0d56a23151da17169225e0cc32fa436e5a4a849293c667863fb876025631a87ab87041037d65941dd4866cd764
tx_blaster_p2p_private_key.operator.teratestnet.staging.2 = ce29af2fa4eb174ea6612cbc7d3bd354e7afea8622c385e84e7a68a5238273ad25f69ac977c9400fb1e4444f1b81c7e21aac680eb3f36a7278a56a5d6cd40db0
tx_blaster_p2p_private_key.operator.teratestnet.staging.3 = d0007fe900755a81635d0203831fe551175896f510f0dc2f8639bac840c4a275731444f003ecfaeed8c0fb172124bb33c7c559a84accd04b2ab2d4ef5ec7a307

# p2p_static_peers is optional, the node will use the bootstrap addresses to find peers regardless
tx_blaster_p2p_static_peers =

tx_blaster_profilerAddr     = :${PROFILE_PORT_TXBLASTER}
tx_blaster_profilerAddr.dev = localhost:${PROFILE_PORT_TXBLASTER}

tx_blaster_staggerWorkersTimeMs = 20

txstore          = null:///
# txstore        = file://./data/txstore?batch=true&writeKeys=true
# txstore.docker = file://./data/txstore
txstore.operator = null:///

# use a batcher by passing batch = true in the query parameters

useLocalValidator                      = false
useLocalValidator.dev.simon            = true
useLocalValidator.docker.host          = true
useLocalValidator.mainnet2             = true
useLocalValidator.operator.mainnet     = true
useLocalValidator.operator.teratestnet = true

# CGO Signer and Verifier Configuration
# -------------------------------------
use_cgo_signer          = true
use_cgo_signer.operator = true

use_cgo_verifier          = true
use_cgo_verifier.operator = true

use_datadog_profiler          = false
use_datadog_profiler.mainnet2 = false

use_open_tracing             = false
use_open_tracing.dev         = true
use_open_tracing.operator    = true
use_open_tracing.docker.host = true
use_open_tracing.docker      = true
use_open_tracing.mainnet2    = true

utxoPersister_buffer_size.operator.mainnet.euw1-1 = 1GB

# we are not using expiration in the operator setup, it seems bugged and we have the space to store all of mainnet. Testing expiration on mainnet-3 only
utxoblaster_utxostore_aerospike          = aerospike://editor:password1234@aerospike-0.ubsv.internal:3000/utxo-store?WarmUp=0&ConnectionQueueSize=640&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=64&expiration=900
utxoblaster_utxostore_aerospike.dev      = aerospike://localhost:3000/test
utxoblaster_utxostore_aerospike.dev.liam = aerospike://localhost:3000/utxo-store?WarmUp=0&ConnectionQueueSize=768&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=76&expiration=900
utxoblaster_utxostore_aerospike.docker.m = aerospike://editor:password1234@aerospike:3000/utxo-store?WarmUp=0&ConnectionQueueSize=640&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=64&expiration=900

utxoblaster_utxostore_redis = redis://${REDIS_HOSTS}

# @group: utxostore compact
dashName.operator.mainnet.euw1-1        = teranode-main-net-1-${regionName}-prod.ubsv.internal:3000
dashName.operator.mainnet.euw1-2        = teranode-main-net-2-${regionName}-prod.ubsv.internal:3000
dashName.operator.mainnet.euw1-3        = teranode-main-net-3-${regionName}-prod.ubsv.internal:3000
dashName.operator.teratestnet.prod.1    = teranode-teranet-1-${regionName}-prod.ubsv.internal:3000
dashName.operator.teratestnet.prod.2    = teranode-teranet-2-${regionName}-prod.ubsv.internal:3000
dashName.operator.teratestnet.prod.3    = teranode-teranet-3-${regionName}-prod.ubsv.internal:3000
dashName.operator.teratestnet.staging.1 = teranode-teranet-1-${regionName}-staging.ubsv.internal:3000
dashName.operator.teratestnet.staging.2 = teranode-teranet-2-${regionName}-staging.ubsv.internal:3000
dashName.operator.teratestnet.staging.3 = teranode-teranet-3-${regionName}-staging.ubsv.internal:3000
# @endgroup

# 54000 # 15 hours - we have the space
utxostore                                      = null:///
utxostore.dev                                  = sqlite:///utxostore?expiration=300
utxostore.dev.legacy                           = aerospike://localhost:3000/test?set=utxo&externalStore=file://./data/external
utxostore.mainnet2                             = aerospike://10.90.0.85:3000,10.90.0.228:3000,10.90.0.253:3000,10.90.0.109:3000,10.90.0.71:3000,10.90.0.100:3000/utxo-store?WarmUp=256&ConnectionQueueSize=256&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=256&set=utxo&externalStore=lustre://s3.eu-central-1.amazonaws.com/teranode-gateway-test-eu-central-1?region=eu-central-1%26subDirectory=sv-node/external%26localDir=/data/ibd/txstore%26localPersist=s3
# utxostore.dev.liam                           = aerospike://localhost:3000/ubsv-store?set=utxo&expiration=4294967295
utxostore.dev.liam                             = postgres://ubsv:ubsv@localhost:5432/ubsv?expiration=300
# utxostore.dev.simon                          = redis://localhost:6379?expiration=5m
utxostore.dev.simon                            = redis2://localhost:6379?expiration=5m&transactionStore=file://./data/transaction_store%3FhashPrefix=2
utxostore.dev.stu                              = postgres://ubsv:ubsv@localhost:5432/ubsv?expiration=300
# utxostore.dev.stu                            = aerospike://localhost:3000/test
utxostore.docker.ci.chainintegrity.ubsv1       = postgres://miner1:miner1@localhost:5432/ubsv1?expiration=300
utxostore.docker.ci.chainintegrity.ubsv2       = postgres://miner2:miner2@localhost:5432/ubsv2?expiration=300
utxostore.docker.ci.chainintegrity.ubsv3       = postgres://miner3:miner3@localhost:5432/ubsv3?expiration=300
utxostore.docker.host.ubsv1.postgres           = postgres://miner${PORT_PREFIX}:miner${PORT_PREFIX}@localhost:15432/ubsv${PORT_PREFIX}?logging=true
utxostore.docker.host                          = aerospike://localhost:3${PORT_PREFIX}00/test?WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=21600&set=utxo&logging=true&externalStore=lustre:///?subDirectory=sv-node/external%26localDir=./data/ubsv${PORT_PREFIX}/external%26localPersist=s3
utxostore.docker.m                             = aerospike://aerospike:3000/test?WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=300
utxostore.docker.ss.ubsv1                      = postgres://miner1:miner1@postgres:5432/ubsv1
utxostore.docker                               = aerospike://${aerospike_host}:${aerospike_port}/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=2&externalStore=file://./data/${clientName}/external
utxostore.docker.ubsv1.test.context.testrunner = aerospike://localhost:13100/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=4294967295&externalStore=file://./../../data/test/ubsv1/external
utxostore.docker.ubsv2.test.context.testrunner = aerospike://localhost:13200/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=4294967295&externalStore=file://./../../data/test/ubsv2/external
utxostore.docker.ubsv3.test.context.testrunner = aerospike://localhost:13300/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=4294967295&externalStore=file://./../../data/test/ubsv3/external
utxostore.docker.ubsv1.context.testrunner      = aerospike://localhost:3100/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=4294967295&externalStore=file://./../../data/ubsv1/external
utxostore.docker.ubsv2.context.testrunner      = aerospike://localhost:3200/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=4294967295&externalStore=file://./../../data/ubsv2/external
utxostore.docker.ubsv3.context.testrunner      = aerospike://localhost:3300/test?set=utxo&WarmUp=32&ConnectionQueueSize=32&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=4294967295&externalStore=file://./../../data/ubsv3/external
utxostore.operator                             = aerospike://editor:password1234@aero-${clientName}.aerospike-${clientName}.svc.cluster.local/utxo-store?set=utxo&WarmUp=0&ConnectionQueueSize=256&LimitConnectionsToQueueSize=true&MinConnectionsPerNode=8&expiration=86400&externalStore=file:///data/${clientName}/external%3FhashPrefix=2

utxostore_dbTimeoutMillis = 5000
# replace utxostore_dbTimeoutMillis with settings.UtxoStore.DBTimeout
utxostore_dbTimeoutDuration = 5s

utxostore_getBatcherDurationMillis          = 100
utxostore_getBatcherDurationMillis.mainnet  = 50
utxostore_getBatcherDurationMillis.mainnet2 = 10
utxostore_getBatcherDurationMillis.operator = 10

utxostore_getBatcherSize          = 100
utxostore_getBatcherSize.mainnet  = 10
utxostore_getBatcherSize.mainnet2 = 8192
utxostore_getBatcherSize.operator = 8192

utxostore_maxMinedBatchSize = 1024

utxostore_maxMinedRoutines          = 4
utxostore_maxMinedRoutines.mainnet2 = 128
utxostore_maxMinedRoutines.operator = 128

utxostore_outpointBatcherConcurrency          = 32
utxostore_outpointBatcherConcurrency.mainnet2 = 128
utxostore_outpointBatcherConcurrency.operator = 128

utxostore_outpointBatcherDurationMillis                         = 100
utxostore_outpointBatcherDurationMillis.mainnet                 = 50
utxostore_outpointBatcherDurationMillis.mainnet2                = 10
utxostore_outpointBatcherDurationMillis.operator                = 10
utxostore_outpointBatcherDurationMillis.operator.mainnet.euw1-2 = 5
utxostore_outpointBatcherDurationMillis.operator.mainnet.euw1-3 = 5

utxostore_outpointBatcherSize                         = 100
utxostore_outpointBatcherSize.mainnet                 = 10
utxostore_outpointBatcherSize.mainnet2                = 8192
utxostore_outpointBatcherSize.operator                = 8192
utxostore_outpointBatcherSize.operator.mainnet.euw1-2 = 1024
utxostore_outpointBatcherSize.operator.mainnet.euw1-3 = 1024

utxostore_spendBatcherConcurrency                  = 32
utxostore_spendBatcherConcurrency.mainnet2         = 128
utxostore_spendBatcherConcurrency.operator.mainnet = 128

utxostore_spendBatcherDurationMillis          = 100
utxostore_spendBatcherDurationMillis.mainnet  = 50
utxostore_spendBatcherDurationMillis.mainnet2 = 10
utxostore_spendBatcherDurationMillis.operator = 10

utxostore_spendBatcherSize          = 100
utxostore_spendBatcherSize.mainnet  = 10
utxostore_spendBatcherSize.mainnet2 = 8192
utxostore_spendBatcherSize.operator = 8192

utxostore_storeBatcherConcurrency          = 32
utxostore_storeBatcherConcurrency.mainnet2 = 128
utxostore_storeBatcherConcurrency.operator = 128

utxostore_storeBatcherDurationMillis          = 100
utxostore_storeBatcherDurationMillis.mainnet  = 50
utxostore_storeBatcherDurationMillis.mainnet2 = 10
utxostore_storeBatcherDurationMillis.operator = 10

utxostore_storeBatcherSize          = 100
utxostore_storeBatcherSize.mainnet  = 10
utxostore_storeBatcherSize.mainnet2 = 1024
utxostore_storeBatcherSize.operator = 1024

# utxos per record
utxostore_utxoBatchSize = 128
utxostore_utxoBatchSize.mainnet2                = 20000
utxostore_utxoBatchSize.operator.mainnet.euw1-3 = 512
utxostore_utxoBatchSize.docker                  = 50

# delay before sending batch to blockvalidation, use for testing subtree processing retries
validator_blockvalidation_delay = 0

# tx validator sending batches to blockvalidation
validator_blockvalidation_maxRetries = 5

validator_blockvalidation_retrySleep = 2s

validator_grpcAddress                 = 0.0.0.0:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker.m        = 0.0.0.0:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker          = ${clientName}:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker.host     = 0.0.0.0:${PORT_PREFIX}${VALIDATOR_GRPC_PORT}
validator_grpcAddress.operator        = k8s:///validator.${clientName}.svc.cluster.local:${VALIDATOR_GRPC_PORT}
validator_grpcAddress.docker.ss.ubsv1 = validator-1:${VALIDATOR_GRPC_PORT}

validator_grpcListenAddress             = :${VALIDATOR_GRPC_PORT}
validator_grpcListenAddress.dev         = localhost:${VALIDATOR_GRPC_PORT}
validator_grpcListenAddress.docker.host = localhost:${PORT_PREFIX}${VALIDATOR_GRPC_PORT}

# turn Kafka off in mainnet2 legacy sync
validator_scriptVerificationLibrary                      = "GoBT"
validator_scriptVerificationLibrary.dev                  = "GoSDK"
validator_scriptVerificationLibrary.mainnet2             = "GoSDK"
validator_scriptVerificationLibrary.operator             = "GoBDK"
validator_scriptVerificationLibrary.operator.teratestnet = "GoSDK"

validator_sendBatchSize                    = 0
validator_sendBatchSize.docker.ubsv1.test.tnb1Test = 10
validator_sendBatchSize.docker.ubsv2.test.tnb1Test = 10
validator_sendBatchSize.docker.ubsv3.test.tnb1Test = 10
validator_sendBatchSize.mainnet2           = 100
validator_sendBatchSize.operator           = 100

validator_sendBatchTimeout = 100

validator_sendBatchWorkers = 1
